Index: linux-2.6.16/include/net/rcp.h
===================================================================
--- linux-2.6.16/include/net/rcp.h	(revision 0)
+++ linux-2.6.16/include/net/rcp.h	(revision 300)
@@ -0,0 +1,40 @@
+/*
+ * Definitions for the RCP module.
+ *
+ * Version:	@(#)rcp.h	1.0.0	07/11/2006
+ *
+ * Authors:	Nandita Dukkipati, <nanditad@Stanford.Edu>
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+#ifndef _RCP_H
+#define	_RCP_H
+
+/* #include <linux/rcp.h> - Include this after adding in the file linux/rcp.h */
+#include <linux/skbuff.h>
+
+#include <net/sock.h>
+#include <net/protocol.h>
+
+/* 
+struct rcp_err {
+  int		errno;
+  unsigned	fatal:1;
+};
+*/
+
+/* extern void	icmp_send(struct sk_buff *skb_in,  int type, int code, u32 * info); */
+extern int	rcp_v4_rcv(struct sk_buff *skb);
+extern int  rcp_queue_xmit(struct sk_buff *skb, int ipfragok);
+void rcp_send_reply(struct sock *sk, struct sk_buff *skb, struct ip_reply_arg *arg, unsigned int len);
+int rcp_build_and_send_pkt(struct sk_buff *skb, struct sock *sk, u32 saddr, u32 daddr, struct ip_options *opt);
+
+/*
+extern int	icmp_ioctl(struct sock *sk, int cmd, unsigned long arg);
+extern void	icmp_init(struct net_proto_family *ops);
+*/
+
+#endif	/* _RCP_H */
Index: linux-2.6.16/include/net/tcp.h
===================================================================
--- linux-2.6.16/include/net/tcp.h	(revision 155)
+++ linux-2.6.16/include/net/tcp.h	(revision 300)
@@ -46,7 +46,9 @@
 extern atomic_t tcp_orphan_count;
 extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 
-#define MAX_TCP_HEADER	(128 + MAX_HEADER)
+/* Nandita: added 16B to make room for RCP Header */
+#define MAX_TCP_HEADER	(128 + MAX_HEADER + 16)
+/* Nandita: end */
 
 /* 
  * Never offer a window over 32767 without using window scaling. Some
@@ -433,6 +435,12 @@
 	inet_csk_clear_xmit_timers(sk);
 }
 
+/* Nandita: Function to compute the packet pacing interval */
+extern unsigned int rcp_tcp_pacing_delta(unsigned int rcp_rate, unsigned int datagram_size); 
+
+/* Nandita: Function to reset pacing timer */
+extern void rcp_tcp_reset_pacing_timer(struct sock *sk);
+
 extern unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);
 extern unsigned int tcp_current_mss(struct sock *sk, int large);
 
Index: linux-2.6.16/include/linux/rcp.h
===================================================================
--- linux-2.6.16/include/linux/rcp.h	(revision 0)
+++ linux-2.6.16/include/linux/rcp.h	(revision 300)
@@ -0,0 +1,27 @@
+/*
+ *
+ * Definitions for the RCP protocol.
+ *
+ * Version:	@(#)rcp.h	1.0.0	07/11/2006
+ *
+ * Authors:	Nandita Dukkipati, <nanditad@Stanford.EDU>
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+#ifndef _LINUX_RCP_H
+#define _LINUX_RCP_H
+
+#define IP_HEADER_SIZE 20
+
+struct rcphdr {
+	__u32	rcp_bottleneck_rate;
+    __u32   rcp_reverse_bottleneck_rate;
+	__u16	rcp_rtt;
+	__u8	protocol;
+	__u8	unused;
+};
+
+#endif	/* _LINUX_RCP_H */
Index: linux-2.6.16/include/linux/in.h
===================================================================
--- linux-2.6.16/include/linux/in.h	(revision 155)
+++ linux-2.6.16/include/linux/in.h	(revision 300)
@@ -45,6 +45,10 @@
   IPPROTO_COMP   = 108,                /* Compression Header protocol */
   IPPROTO_SCTP   = 132,		/* Stream Control Transport Protocol	*/
 
+  /* Nandita: start */
+  IPPROTO_RCP    = 254,     /* Rate Control Protocol */
+  /* Nandita: end */
+
   IPPROTO_RAW	 = 255,		/* Raw IP packets			*/
   IPPROTO_MAX
 };
Index: linux-2.6.16/include/linux/tcp.h
===================================================================
--- linux-2.6.16/include/linux/tcp.h	(revision 155)
+++ linux-2.6.16/include/linux/tcp.h	(revision 300)
@@ -343,6 +343,25 @@
 		__u32	seq;
 		__u32	time;
 	} rcvq_space;
+
+/* Nandita start: RCP information that is updated on every incoming packet and is
+ * carried on every outgoing packet */
+    __u32 rcp_bottleneck_rate;
+    __u32 rcp_reverse_bottleneck_rate;
+    __u16 rcp_rtt;
+
+    __u16 rcp_minRTT;
+    __u16 rcp_sRTT;
+
+/* Used for pacing packets */ 
+    __u32 rcp_pacing_interval;              /* Time interval (in jiffies) between two RCP data packets */
+    __u32 rcp_last_packet_transmitted_time; /* Time when the last RCP data packet was transmitted. */
+    struct timer_list rcp_pacing_timer;     /* Timer for pacing RCP packets */
+    int rcp_pacing;                        /* Should RCP packets be paced ? */ 
+    int rcp_pacing_lock;
+
+/* Nandita: End */
+    
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
Index: linux-2.6.16/net/ipv4/tcp_input.c
===================================================================
--- linux-2.6.16/net/ipv4/tcp_input.c	(revision 155)
+++ linux-2.6.16/net/ipv4/tcp_input.c	(revision 300)
@@ -72,6 +72,9 @@
 #include <linux/ipsec.h>
 #include <asm/unaligned.h>
 
+/* Nandita */
+#include <linux/rcp.h>
+
 int sysctl_tcp_timestamps = 1;
 int sysctl_tcp_window_scaling = 1;
 int sysctl_tcp_sack = 1;
@@ -112,6 +115,10 @@
 
 #define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)
 
+/* Nandita */
+#define debug_tcp_input 0 
+#define debug_rtt       0 
+
 /* Adapt the MSS value used to make delayed ack decision to the 
  * real world.
  */ 
@@ -122,6 +129,9 @@
 	const unsigned int lss = icsk->icsk_ack.last_seg_size; 
 	unsigned int len;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_measure_rcv_mss [tcp_input.c]\n");
+
 	icsk->icsk_ack.last_seg_size = 0; 
 
 	/* skb->len may jitter because of SACKs, even if peer
@@ -164,6 +174,9 @@
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	unsigned quickacks = tcp_sk(sk)->rcv_wnd / (2 * icsk->icsk_ack.rcv_mss);
+    
+    if (debug_tcp_input)
+        printk(" *** tcp_incr_quickack [tcp_input.c]\n");
 
 	if (quickacks==0)
 		quickacks=2;
@@ -174,6 +187,10 @@
 void tcp_enter_quickack_mode(struct sock *sk)
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_enter_quickack_mode [tcp_input.c]\n");
+
 	tcp_incr_quickack(sk);
 	icsk->icsk_ack.pingpong = 0;
 	icsk->icsk_ack.ato = TCP_ATO_MIN;
@@ -186,6 +203,9 @@
 static inline int tcp_in_quickack_mode(const struct sock *sk)
 {
 	const struct inet_connection_sock *icsk = inet_csk(sk);
+    if (debug_tcp_input)
+        printk(" *** tcp_in_quickack_mode [tcp_input.c]\n");
+    
 	return icsk->icsk_ack.quick && !icsk->icsk_ack.pingpong;
 }
 
@@ -199,6 +219,9 @@
 	int sndmem = tcp_sk(sk)->rx_opt.mss_clamp + MAX_TCP_HEADER + 16 +
 		     sizeof(struct sk_buff);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_fixup_sndbuf [tcp_input.c]\n");
+
 	if (sk->sk_sndbuf < 3 * sndmem)
 		sk->sk_sndbuf = min(3 * sndmem, sysctl_tcp_wmem[2]);
 }
@@ -236,6 +259,9 @@
 	int truesize = tcp_win_from_space(skb->truesize)/2;
 	int window = tcp_win_from_space(sysctl_tcp_rmem[2])/2;
 
+    if (debug_tcp_input)
+        printk(" *** _tcp_grow_window [tcp_input.c]\n");
+
 	while (tp->rcv_ssthresh <= window) {
 		if (truesize <= skb->len)
 			return 2 * inet_csk(sk)->icsk_ack.rcv_mss;
@@ -249,6 +275,9 @@
 static void tcp_grow_window(struct sock *sk, struct tcp_sock *tp,
 			    struct sk_buff *skb)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_grow_window [tcp_input.c]\n");
 	/* Check #1 */
 	if (tp->rcv_ssthresh < tp->window_clamp &&
 	    (int)tp->rcv_ssthresh < tcp_space(sk) &&
@@ -277,6 +306,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	int rcvmem = tp->advmss + MAX_TCP_HEADER + 16 + sizeof(struct sk_buff);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_fixup_rcvbuf [tcp_input.c]\n");
+
 	/* Try to select rcvbuf so that 4 mss-sized segments
 	 * will fit to window and corresponding skbs will fit to our rcvbuf.
 	 * (was 3; 4 is minimum to allow fast retransmit to work.)
@@ -295,6 +327,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	int maxwin;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_init_buffer_space [tcp_input.c]\n");
+
 	if (!(sk->sk_userlocks & SOCK_RCVBUF_LOCK))
 		tcp_fixup_rcvbuf(sk);
 	if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK))
@@ -328,6 +363,9 @@
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_clamp_window [tcp_input.c]\n");
+
 	icsk->icsk_ack.quick = 0;
 
 	if (sk->sk_rcvbuf < sysctl_tcp_rmem[2] &&
@@ -354,6 +392,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	unsigned int hint = min_t(unsigned int, tp->advmss, tp->mss_cache);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_initialize_rcv_mss [tcp_input.c]\n");
+
 	hint = min(hint, tp->rcv_wnd/2);
 	hint = min(hint, TCP_MIN_RCVMSS);
 	hint = max(hint, TCP_MIN_MSS);
@@ -377,6 +418,9 @@
 	u32 new_sample = tp->rcv_rtt_est.rtt;
 	long m = sample;
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_rcv_rtt_update [tcp_input.c]\n");
+
 	if (m == 0)
 		m = 1;
 
@@ -407,6 +451,9 @@
 
 static inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)
 {
+    if (debug_tcp_input || debug_rtt)
+         printk(" *** tcp_rcv_rtt_measure [tcp_input.c]\n");
+
 	if (tp->rcv_rtt_est.time == 0)
 		goto new_measure;
 	if (before(tp->rcv_nxt, tp->rcv_rtt_est.seq))
@@ -423,6 +470,10 @@
 static inline void tcp_rcv_rtt_measure_ts(struct sock *sk, const struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_input || debug_rtt)
+         printk(" *** tcp_rcv_rtt_measure_ts [tcp_input.c]\n");
+
 	if (tp->rx_opt.rcv_tsecr &&
 	    (TCP_SKB_CB(skb)->end_seq -
 	     TCP_SKB_CB(skb)->seq >= inet_csk(sk)->icsk_ack.rcv_mss))
@@ -438,6 +489,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	int time;
 	int space;
+
+    if (debug_tcp_input)
+        printk(" *** tcp_rcv_space_adjust [tcp_input.c]\n");
 	
 	if (tp->rcvq_space.time == 0)
 		goto new_measure;
@@ -502,6 +556,9 @@
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	u32 now;
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_event_data_recv [tcp_input.c]\n");
+
 	inet_csk_schedule_ack(sk);
 
 	tcp_measure_rcv_mss(sk, skb);
@@ -556,6 +613,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	long m = mrtt; /* RTT */
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_rtt_estimator [tcp_input.c]\n");
+
 	/*	The following amusing code comes from Jacobson's
 	 *	article in SIGCOMM '88.  Note that rtt and mdev
 	 *	are scaled versions of rtt and mean deviation.
@@ -620,6 +680,9 @@
 static inline void tcp_set_rto(struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_set_rto [tcp_input.c]\n");
 	/* Old crap is replaced with new one. 8)
 	 *
 	 * More seriously:
@@ -644,6 +707,8 @@
  */
 static inline void tcp_bound_rto(struct sock *sk)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_bound_rto [tcp_input.c]\n");
 	if (inet_csk(sk)->icsk_rto > TCP_RTO_MAX)
 		inet_csk(sk)->icsk_rto = TCP_RTO_MAX;
 }
@@ -657,6 +722,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct dst_entry *dst = __sk_dst_get(sk);
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_update_metrics [tcp_input.c]\n");
+
 	if (sysctl_tcp_nometrics_save)
 		return;
 
@@ -747,13 +815,24 @@
 {
 	__u32 cwnd = (dst ? dst_metric(dst, RTAX_INITCWND) : 0);
 
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_init_cwnd [tcp_input.c]\n");
+
 	if (!cwnd) {
 		if (tp->mss_cache > 1460)
 			cwnd = 2;
 		else
 			cwnd = (tp->mss_cache > 1095) ? 3 : 4;
 	}
-	return min_t(__u32, cwnd, tp->snd_cwnd_clamp);
+
+    if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) {
+        cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+        return (cwnd);
+    } else 
+	    return min_t(__u32, cwnd, tp->snd_cwnd_clamp);
 }
 
 /* Set slow start threshold and cwnd not falling to slow start */
@@ -761,13 +840,25 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_enter_cwr [tcp_input.c]\n");
+
 	tp->prior_ssthresh = 0;
 	tp->bytes_acked = 0;
 	if (inet_csk(sk)->icsk_ca_state < TCP_CA_CWR) {
 		tp->undo_marker = 0;
 		tp->snd_ssthresh = inet_csk(sk)->icsk_ca_ops->ssthresh(sk);
-		tp->snd_cwnd = min(tp->snd_cwnd,
-				   tcp_packets_in_flight(tp) + 1U);
+
+        /* Nandita */
+        if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+            tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+        else
+		    tp->snd_cwnd = min(tp->snd_cwnd, tcp_packets_in_flight(tp) + 1U);
+
 		tp->snd_cwnd_cnt = 0;
 		tp->high_seq = tp->snd_nxt;
 		tp->snd_cwnd_stamp = tcp_time_stamp;
@@ -784,6 +875,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct dst_entry *dst = __sk_dst_get(sk);
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_init_metrics [tcp_input.c]\n");
+
 	if (dst == NULL)
 		goto reset;
 
@@ -822,6 +916,7 @@
 	 * to low value, and then abruptly stops to do it and starts to delay
 	 * ACKs, wait for troubles.
 	 */
+
 	if (dst_metric(dst, RTAX_RTT) > tp->srtt) {
 		tp->srtt = dst_metric(dst, RTAX_RTT);
 		tp->rtt_seq = tp->snd_nxt;
@@ -854,6 +949,10 @@
 				  const int ts)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_update_reordering [tcp_input.c]\n");
+
 	if (metric > tp->reordering) {
 		tp->reordering = min(TCP_MAX_REORDERING, metric);
 
@@ -942,6 +1041,9 @@
 	int dup_sack = 0;
 	int i;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_sacktag_write_queue [tcp_input.c]\n");
+
 	if (!tp->sacked_out)
 		tp->fackets_out = 0;
 	prior_fackets = tp->fackets_out;
@@ -1228,6 +1330,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *skb;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_enter_frto [tcp_input.c]\n");
+
 	tp->frto_counter = 1;
 
 	if (icsk->icsk_ca_state <= TCP_CA_Disorder ||
@@ -1266,6 +1371,13 @@
 	struct sk_buff *skb;
 	int cnt = 0;
 
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_enter_frto_loss [tcp_input.c]\n");
+
 	tp->sacked_out = 0;
 	tp->lost_out = 0;
 	tp->fackets_out = 0;
@@ -1290,7 +1402,12 @@
 	}
 	tcp_sync_left_out(tp);
 
-	tp->snd_cwnd = tp->frto_counter + tcp_packets_in_flight(tp)+1;
+    /* Nandita */
+    if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+    else 
+	    tp->snd_cwnd = tp->frto_counter + tcp_packets_in_flight(tp)+1;
+
 	tp->snd_cwnd_cnt = 0;
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 	tp->undo_marker = 0;
@@ -1307,6 +1424,10 @@
 
 void tcp_clear_retrans(struct tcp_sock *tp)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_clear_retrans [tcp_input.c]\n");
+
 	tp->left_out = 0;
 	tp->retrans_out = 0;
 
@@ -1329,6 +1450,13 @@
 	struct sk_buff *skb;
 	int cnt = 0;
 
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_enter_loss [tcp_input.c]\n");
+
 	/* Reduce ssthresh if it has not yet been made inside this window. */
 	if (icsk->icsk_ca_state <= TCP_CA_Disorder || tp->snd_una == tp->high_seq ||
 	    (icsk->icsk_ca_state == TCP_CA_Loss && !icsk->icsk_retransmits)) {
@@ -1336,7 +1464,13 @@
 		tp->snd_ssthresh = icsk->icsk_ca_ops->ssthresh(sk);
 		tcp_ca_event(sk, CA_EVENT_LOSS);
 	}
-	tp->snd_cwnd	   = 1;
+
+    /* Nandita */ 
+    if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+    else
+	    tp->snd_cwnd   = 1;
+
 	tp->snd_cwnd_cnt   = 0;
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 
@@ -1377,6 +1511,9 @@
 {
 	struct sk_buff *skb;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_check_sack_reneging [tcp_input.c]\n");
+
 	/* If ACK arrived pointing to a remembered SACK,
 	 * it means that our remembered SACKs do not reflect
 	 * real state of receiver i.e.
@@ -1400,16 +1537,22 @@
 
 static inline int tcp_fackets_out(struct tcp_sock *tp)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_fackets_out [tcp_input.c]\n");
 	return IsReno(tp) ? tp->sacked_out+1 : tp->fackets_out;
 }
 
 static inline int tcp_skb_timedout(struct sock *sk, struct sk_buff *skb)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_skb_timedout [tcp_input.c]\n");
 	return (tcp_time_stamp - TCP_SKB_CB(skb)->when > inet_csk(sk)->icsk_rto);
 }
 
 static inline int tcp_head_timedout(struct sock *sk, struct tcp_sock *tp)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_head_timedout [tcp_input.c]\n");
 	return tp->packets_out &&
 	       tcp_skb_timedout(sk, skb_peek(&sk->sk_write_queue));
 }
@@ -1511,6 +1654,9 @@
 {
 	__u32 packets_out;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_time_to_recover [tcp_input.c]\n");
+
 	/* Trick#1: The loss is proven. */
 	if (tp->lost_out)
 		return 1;
@@ -1550,6 +1696,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	u32 holes;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_check_reno_reordering [tcp_input.c]\n");
+
 	holes = max(tp->lost_out, 1U);
 	holes = min(holes, tp->packets_out);
 
@@ -1564,6 +1713,10 @@
 static void tcp_add_reno_sack(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_add_reno_sack [tcp_input.c]\n");
+    
 	tp->sacked_out++;
 	tcp_check_reno_reordering(sk, 0);
 	tcp_sync_left_out(tp);
@@ -1573,6 +1726,9 @@
 
 static void tcp_remove_reno_sacks(struct sock *sk, struct tcp_sock *tp, int acked)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_remove_reno_sacks [tcp_input.c]\n");
 	if (acked > 0) {
 		/* One ACK acked hole. The rest eat duplicate ACKs. */
 		if (acked-1 >= tp->sacked_out)
@@ -1586,6 +1742,8 @@
 
 static inline void tcp_reset_reno_sack(struct tcp_sock *tp)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_reset_reno_sack [tcp_input.c]\n");
 	tp->sacked_out = 0;
 	tp->left_out = tp->lost_out;
 }
@@ -1597,6 +1755,9 @@
 	struct sk_buff *skb;
 	int cnt;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_mark_head_lost [tcp_input.c]\n");
+
 	BUG_TRAP(packets <= tp->packets_out);
 	if (tp->lost_skb_hint) {
 		skb = tp->lost_skb_hint;
@@ -1635,6 +1796,10 @@
 
 static void tcp_update_scoreboard(struct sock *sk, struct tcp_sock *tp)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_update_scoreboard [tcp_input.c]\n");
+
 	if (IsFack(tp)) {
 		int lost = tp->fackets_out - tp->reordering;
 		if (lost <= 0)
@@ -1683,8 +1848,18 @@
  */
 static inline void tcp_moderate_cwnd(struct tcp_sock *tp)
 {
-	tp->snd_cwnd = min(tp->snd_cwnd,
-			   tcp_packets_in_flight(tp)+tcp_max_burst(tp));
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_moderate_cwnd [tcp_input.c]\n");
+
+    /* Nandita */
+    if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+    else
+	    tp->snd_cwnd = min(tp->snd_cwnd, tcp_packets_in_flight(tp)+tcp_max_burst(tp));
+
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 }
 
@@ -1695,13 +1870,25 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	int decr = tp->snd_cwnd_cnt + 1;
 
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_cwnd_down [tcp_input.c]\n");
+
 	tp->snd_cwnd_cnt = decr&1;
 	decr >>= 1;
 
-	if (decr && tp->snd_cwnd > icsk->icsk_ca_ops->min_cwnd(sk))
-		tp->snd_cwnd -= decr;
+    if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) 
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+    else {
+	    if (decr && tp->snd_cwnd > icsk->icsk_ca_ops->min_cwnd(sk))
+		    tp->snd_cwnd -= decr;
 
-	tp->snd_cwnd = min(tp->snd_cwnd, tcp_packets_in_flight(tp)+1);
+    	tp->snd_cwnd = min(tp->snd_cwnd, tcp_packets_in_flight(tp)+1);
+    }
+
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 }
 
@@ -1710,6 +1897,10 @@
  */
 static inline int tcp_packet_delayed(struct tcp_sock *tp)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_packet_delayed [tcp_input.c]\n");
+
 	return !tp->retrans_stamp ||
 		(tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&
 		 (__s32)(tp->rx_opt.rcv_tsecr - tp->retrans_stamp) < 0);
@@ -1736,20 +1927,34 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_undo_cwr [tcp_input.c]\n");
+
 	if (tp->prior_ssthresh) {
 		const struct inet_connection_sock *icsk = inet_csk(sk);
 
-		if (icsk->icsk_ca_ops->undo_cwnd)
-			tp->snd_cwnd = icsk->icsk_ca_ops->undo_cwnd(sk);
-		else
-			tp->snd_cwnd = max(tp->snd_cwnd, tp->snd_ssthresh<<1);
+        if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) 
+            tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+        else {
+		    if (icsk->icsk_ca_ops->undo_cwnd)
+			    tp->snd_cwnd = icsk->icsk_ca_ops->undo_cwnd(sk);
+		    else
+			    tp->snd_cwnd = max(tp->snd_cwnd, tp->snd_ssthresh<<1);
+        }
 
 		if (undo && tp->prior_ssthresh > tp->snd_ssthresh) {
 			tp->snd_ssthresh = tp->prior_ssthresh;
 			TCP_ECN_withdraw_cwr(tp);
 		}
 	} else {
-		tp->snd_cwnd = max(tp->snd_cwnd, tp->snd_ssthresh);
+        if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+            tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+        else
+		    tp->snd_cwnd = max(tp->snd_cwnd, tp->snd_ssthresh);
 	}
 	tcp_moderate_cwnd(tp);
 	tp->snd_cwnd_stamp = tcp_time_stamp;
@@ -1768,6 +1973,10 @@
 /* People celebrate: "We love our President!" */
 static int tcp_try_undo_recovery(struct sock *sk, struct tcp_sock *tp)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_try_undo_recovery [tcp_input.c]\n");
+
 	if (tcp_may_undo(tp)) {
 		/* Happy end! We did not retransmit anything
 		 * or our original transmission succeeded.
@@ -1794,6 +2003,9 @@
 /* Try to undo cwnd reduction, because D-SACKs acked all retransmitted data */
 static void tcp_try_undo_dsack(struct sock *sk, struct tcp_sock *tp)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_try_undo_dsack [tcp_input.c]\n");
+
 	if (tp->undo_marker && !tp->undo_retrans) {
 		DBGUNDO(sk, tp, "D-SACK");
 		tcp_undo_cwr(sk, 1);
@@ -1810,6 +2022,9 @@
 	/* Partial ACK arrived. Force Hoe's retransmit. */
 	int failed = IsReno(tp) || tp->fackets_out>tp->reordering;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_try_undo_partial [tcp_input.c]\n");
+
 	if (tcp_may_undo(tp)) {
 		/* Plain luck! Hole if filled with delayed
 		 * packet, rather than with a retransmit.
@@ -1835,6 +2050,10 @@
 /* Undo during loss recovery after partial ACK. */
 static int tcp_try_undo_loss(struct sock *sk, struct tcp_sock *tp)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_try_undo_loss [tcp_input.c]\n");
+
 	if (tcp_may_undo(tp)) {
 		struct sk_buff *skb;
 		sk_stream_for_retrans_queue(skb, sk) {
@@ -1860,13 +2079,29 @@
 static inline void tcp_complete_cwr(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
-	tp->snd_cwnd = min(tp->snd_cwnd, tp->snd_ssthresh);
+
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_complete_cwr [tcp_input.c]\n");
+    
+    /* Nandita */
+    if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+    else
+	    tp->snd_cwnd = min(tp->snd_cwnd, tp->snd_ssthresh);
+
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 	tcp_ca_event(sk, CA_EVENT_COMPLETE_CWR);
 }
 
 static void tcp_try_to_open(struct sock *sk, struct tcp_sock *tp, int flag)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_try_to_open [tcp_input.c]\n");
 	tp->left_out = tp->sacked_out;
 
 	if (tp->retrans_out == 0)
@@ -1910,6 +2145,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	int is_dupack = (tp->snd_una == prior_snd_una && !(flag&FLAG_NOT_DUP));
 
+    if (debug_tcp_input)
+        printk(" *** tcp_fastretrans_alert [tcp_input.c]\n");
+
 	/* Some technical things:
 	 * 1. Reno does not count dupacks (sacked_out) automatically. */
 	if (!tp->packets_out)
@@ -2075,6 +2313,10 @@
 	 */
 	struct tcp_sock *tp = tcp_sk(sk);
 	const __u32 seq_rtt = tcp_time_stamp - tp->rx_opt.rcv_tsecr;
+
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_ack_saw_tstamp [tcp_input.c]\n");
+
 	tcp_rtt_estimator(sk, seq_rtt);
 	tcp_set_rto(sk);
 	inet_csk(sk)->icsk_backoff = 0;
@@ -2092,6 +2334,9 @@
 	 * I.e. Karn's algorithm. (SIGCOMM '87, p5.)
 	 */
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_ack_no_tstamp [tcp_input.c]\n");
+
 	if (flag & FLAG_RETRANS_DATA_ACKED)
 		return;
 
@@ -2105,6 +2350,10 @@
 				      const s32 seq_rtt)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_ack_update_rtt [tcp_input.c]\n");
+    
 	/* Note that peer MAY send zero echo. In this case it is ignored. (rfc1323) */
 	if (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)
 		tcp_ack_saw_tstamp(sk, flag);
@@ -2116,6 +2365,9 @@
 			   u32 in_flight, int good)
 {
 	const struct inet_connection_sock *icsk = inet_csk(sk);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_cong_avoid [tcp_input.c]\n");
 	icsk->icsk_ca_ops->cong_avoid(sk, ack, rtt, in_flight, good);
 	tcp_sk(sk)->snd_cwnd_stamp = tcp_time_stamp;
 }
@@ -2126,6 +2378,9 @@
 
 static void tcp_ack_packets_out(struct sock *sk, struct tcp_sock *tp)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_ack_packets_out [tcp_input.c]\n");
+
 	if (!tp->packets_out) {
 		inet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);
 	} else {
@@ -2142,6 +2397,9 @@
 	__u32 packets_acked;
 	int acked = 0;
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_tso_acked [tcp_input.c]\n");
+
 	/* If we get here, the whole TSO packet has not been
 	 * acked.
 	 */
@@ -2193,6 +2451,9 @@
 {
 	struct timeval tv, now;
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_usrtt [tcp_input.c]\n");
+
 	do_gettimeofday(&now);
 	skb_get_timestamp(skb, &tv);
 	return (now.tv_sec - tv.tv_sec) * 1000000 + (now.tv_usec - tv.tv_usec);
@@ -2211,6 +2472,9 @@
 	void (*rtt_sample)(struct sock *sk, u32 usrtt)
 		= icsk->icsk_ca_ops->rtt_sample;
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_clean_rtx_queue [tcp_input.c]\n");
+
 	while ((skb = skb_peek(&sk->sk_write_queue)) &&
 	       skb != sk->sk_send_head) {
 		struct tcp_skb_cb *scb = TCP_SKB_CB(skb); 
@@ -2315,6 +2579,8 @@
 	const struct tcp_sock *tp = tcp_sk(sk);
 	struct inet_connection_sock *icsk = inet_csk(sk);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_ack_probe [tcp_input.c]\n");
 	/* Was it a usable window open? */
 
 	if (!after(TCP_SKB_CB(sk->sk_send_head)->end_seq,
@@ -2333,6 +2599,8 @@
 
 static inline int tcp_ack_is_dubious(const struct sock *sk, const int flag)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_ack_is_dubious [tcp_input.c]\n");
 	return (!(flag & FLAG_NOT_DUP) || (flag & FLAG_CA_ALERT) ||
 		inet_csk(sk)->icsk_ca_state != TCP_CA_Open);
 }
@@ -2340,6 +2608,8 @@
 static inline int tcp_may_raise_cwnd(const struct sock *sk, const int flag)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
+    if (debug_tcp_input)
+        printk(" *** tcp_may_raise_cwnd [tcp_input.c]\n");
 	return (!(flag & FLAG_ECE) || tp->snd_cwnd < tp->snd_ssthresh) &&
 		!((1 << inet_csk(sk)->icsk_ca_state) & (TCPF_CA_Recovery | TCPF_CA_CWR));
 }
@@ -2350,6 +2620,8 @@
 static inline int tcp_may_update_window(const struct tcp_sock *tp, const u32 ack,
 					const u32 ack_seq, const u32 nwin)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_may_update_window [tcp_input.c]\n");
 	return (after(ack, tp->snd_una) ||
 		after(ack_seq, tp->snd_wl1) ||
 		(ack_seq == tp->snd_wl1 && nwin > tp->snd_wnd));
@@ -2366,6 +2638,9 @@
 	int flag = 0;
 	u32 nwin = ntohs(skb->h.th->window);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_ack_update_window [tcp_input.c]\n");
+
 	if (likely(!skb->h.th->syn))
 		nwin <<= tp->rx_opt.snd_wscale;
 
@@ -2397,6 +2672,13 @@
 static void tcp_process_frto(struct sock *sk, u32 prior_snd_una)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_process_frto [tcp_input.c]\n");
 	
 	tcp_sync_left_out(tp);
 	
@@ -2413,13 +2695,21 @@
 		/* First ACK after RTO advances the window: allow two new
 		 * segments out.
 		 */
-		tp->snd_cwnd = tcp_packets_in_flight(tp) + 2;
+        if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+            tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+        else
+		    tp->snd_cwnd = tcp_packets_in_flight(tp) + 2;
+
 	} else {
 		/* Also the second ACK after RTO advances the window.
 		 * The RTO was likely spurious. Reduce cwnd and continue
 		 * in congestion avoidance
 		 */
-		tp->snd_cwnd = min(tp->snd_cwnd, tp->snd_ssthresh);
+        if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+            tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+        else
+		    tp->snd_cwnd = min(tp->snd_cwnd, tp->snd_ssthresh);
+
 		tcp_moderate_cwnd(tp);
 	}
 
@@ -2441,6 +2731,9 @@
 	s32 seq_rtt;
 	int prior_packets;
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_ack [tcp_input.c]\n");
+
 	/* If the ack is newer than sent or older than previous acks
 	 * then we can probably ignore it.
 	 */
@@ -2545,6 +2838,9 @@
 	struct tcphdr *th = skb->h.th;
 	int length=(th->doff*4)-sizeof(struct tcphdr);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_parse_options [tcp_input.c]\n");
+
 	ptr = (unsigned char *)(th + 1);
 	opt_rx->saw_tstamp = 0;
 
@@ -2628,6 +2924,9 @@
 static int tcp_fast_parse_options(struct sk_buff *skb, struct tcphdr *th,
 				  struct tcp_sock *tp)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_fast_parse_options [tcp_input.c]\n");
 	if (th->doff == sizeof(struct tcphdr)>>2) {
 		tp->rx_opt.saw_tstamp = 0;
 		return 0;
@@ -2650,12 +2949,16 @@
 
 static inline void tcp_store_ts_recent(struct tcp_sock *tp)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_store_ts_recent [tcp_input.c]\n");
 	tp->rx_opt.ts_recent = tp->rx_opt.rcv_tsval;
 	tp->rx_opt.ts_recent_stamp = xtime.tv_sec;
 }
 
 static inline void tcp_replace_ts_recent(struct tcp_sock *tp, u32 seq)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_replace_ts_recent [tcp_input.c]\n");
 	if (tp->rx_opt.saw_tstamp && !after(seq, tp->rcv_wup)) {
 		/* PAWS bug workaround wrt. ACK frames, the PAWS discard
 		 * extra check below makes sure this can only happen
@@ -2700,6 +3003,9 @@
 	u32 seq = TCP_SKB_CB(skb)->seq;
 	u32 ack = TCP_SKB_CB(skb)->ack_seq;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_disordered_ack [tcp_input.c]\n");
+
 	return (/* 1. Pure ACK with correct sequence number. */
 		(th->ack && seq == TCP_SKB_CB(skb)->end_seq && seq == tp->rcv_nxt) &&
 
@@ -2716,6 +3022,8 @@
 static inline int tcp_paws_discard(const struct sock *sk, const struct sk_buff *skb)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
+    if (debug_tcp_input)
+        printk(" *** tcp_paws_discard [tcp_input.c]\n");
 	return ((s32)(tp->rx_opt.ts_recent - tp->rx_opt.rcv_tsval) > TCP_PAWS_WINDOW &&
 		xtime.tv_sec < tp->rx_opt.ts_recent_stamp + TCP_PAWS_24DAYS &&
 		!tcp_disordered_ack(sk, skb));
@@ -2736,6 +3044,8 @@
 
 static inline int tcp_sequence(struct tcp_sock *tp, u32 seq, u32 end_seq)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_sequence [tcp_input.c]\n");
 	return	!before(end_seq, tp->rcv_wup) &&
 		!after(seq, tp->rcv_nxt + tcp_receive_window(tp));
 }
@@ -2743,6 +3053,8 @@
 /* When we get a reset we do this. */
 static void tcp_reset(struct sock *sk)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_reset [tcp_input.c]\n");
 	/* We want the right error as BSD sees it (and indeed as we do). */
 	switch (sk->sk_state) {
 		case TCP_SYN_SENT:
@@ -2781,6 +3093,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_fin [tcp_input.c]\n");
+
 	inet_csk_schedule_ack(sk);
 
 	sk->sk_shutdown |= RCV_SHUTDOWN;
@@ -2848,6 +3163,9 @@
 
 static inline int tcp_sack_extend(struct tcp_sack_block *sp, u32 seq, u32 end_seq)
 {
+
+    if (debug_tcp_input)
+        printk(" *** tcp_sack_extend [tcp_input.c]\n");
 	if (!after(seq, sp->end_seq) && !after(sp->start_seq, end_seq)) {
 		if (before(seq, sp->start_seq))
 			sp->start_seq = seq;
@@ -2860,6 +3178,8 @@
 
 static void tcp_dsack_set(struct tcp_sock *tp, u32 seq, u32 end_seq)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_dsack_set [tcp_input.c]\n");
 	if (tp->rx_opt.sack_ok && sysctl_tcp_dsack) {
 		if (before(seq, tp->rcv_nxt))
 			NET_INC_STATS_BH(LINUX_MIB_TCPDSACKOLDSENT);
@@ -2875,6 +3195,8 @@
 
 static void tcp_dsack_extend(struct tcp_sock *tp, u32 seq, u32 end_seq)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_dsack_extend [tcp_input.c]\n");
 	if (!tp->rx_opt.dsack)
 		tcp_dsack_set(tp, seq, end_seq);
 	else
@@ -2885,6 +3207,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_send_dupack [tcp_input.c]\n");
+
 	if (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&
 	    before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {
 		NET_INC_STATS_BH(LINUX_MIB_DELAYEDACKLOST);
@@ -2911,6 +3236,9 @@
 	struct tcp_sack_block *sp = &tp->selective_acks[0];
 	struct tcp_sack_block *swalk = sp+1;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_sack_maybe_coalesce [tcp_input.c]\n");
+
 	/* See if the recent change to the first SACK eats into
 	 * or hits the sequence space of other SACK blocks, if so coalesce.
 	 */
@@ -2935,6 +3263,9 @@
 {
 	__u32 tmp;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_sack_swap [tcp_input.c]\n");
+
 	tmp = sack1->start_seq;
 	sack1->start_seq = sack2->start_seq;
 	sack2->start_seq = tmp;
@@ -2951,6 +3282,9 @@
 	int cur_sacks = tp->rx_opt.num_sacks;
 	int this_sack;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_sack_new_ofo_skb [tcp_input.c]\n");
+
 	if (!cur_sacks)
 		goto new_sack;
 
@@ -2995,6 +3329,9 @@
 	int num_sacks = tp->rx_opt.num_sacks;
 	int this_sack;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_sack_remove [tcp_input.c]\n");
+
 	/* Empty ofo queue, hence, all the SACKs are eaten. Clear. */
 	if (skb_queue_empty(&tp->out_of_order_queue)) {
 		tp->rx_opt.num_sacks = 0;
@@ -3034,6 +3371,9 @@
 	__u32 dsack_high = tp->rcv_nxt;
 	struct sk_buff *skb;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_ofo_queue [tcp_input.c]\n");
+
 	while ((skb = skb_peek(&tp->out_of_order_queue)) != NULL) {
 		if (after(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))
 			break;
@@ -3071,6 +3411,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	int eaten = -1;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_data_queue [tcp_input.c]\n");
+
 	if (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq)
 		goto drop;
 
@@ -3288,6 +3631,9 @@
 {
 	struct sk_buff *skb;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_collapse [tcp_input.c]\n");
+
 	/* First, check that queue is collapsible and find
 	 * the point where collapsing can be useful. */
 	for (skb = head; skb != tail; ) {
@@ -3380,6 +3726,9 @@
 	struct sk_buff *head;
 	u32 start, end;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_collapse_ofo_queue [tcp_input.c]\n");
+
 	if (skb == NULL)
 		return;
 
@@ -3423,6 +3772,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk); 
 
+    if (debug_tcp_input)
+        printk(" *** tcp_prune_queue [tcp_input.c]\n");
+
 	SOCK_DEBUG(sk, "prune_queue: c=%x\n", tp->copied_seq);
 
 	NET_INC_STATS_BH(LINUX_MIB_PRUNECALLED);
@@ -3483,13 +3835,24 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_input)
+        printk(" *** tcp_cwnd_application_limited [tcp_input.c]\n");
+
 	if (inet_csk(sk)->icsk_ca_state == TCP_CA_Open &&
 	    sk->sk_socket && !test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {
 		/* Limited by application or receiver window. */
 		u32 win_used = max(tp->snd_cwnd_used, 2U);
 		if (win_used < tp->snd_cwnd) {
 			tp->snd_ssthresh = tcp_current_ssthresh(sk);
-			tp->snd_cwnd = (tp->snd_cwnd + win_used) >> 1;
+
+            if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+                tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+            else
+			    tp->snd_cwnd = (tp->snd_cwnd + win_used) >> 1;
 		}
 		tp->snd_cwnd_used = 0;
 	}
@@ -3501,6 +3864,10 @@
 	/* If the user specified a specific send buffer setting, do
 	 * not modify it.
 	 */
+
+    if (debug_tcp_input)
+        printk(" *** tcp_should_expand_sndbuf [tcp_input.c]\n");
+
 	if (sk->sk_userlocks & SOCK_SNDBUF_LOCK)
 		return 0;
 
@@ -3529,6 +3896,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_new_space [tcp_input.c]\n");
+
 	if (tcp_should_expand_sndbuf(sk, tp)) {
  		int sndmem = max_t(u32, tp->rx_opt.mss_clamp, tp->mss_cache) +
 			MAX_TCP_HEADER + 16 + sizeof(struct sk_buff),
@@ -3545,6 +3915,9 @@
 
 static void tcp_check_space(struct sock *sk)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_check_space [tcp_input.c]\n");
+
 	if (sock_flag(sk, SOCK_QUEUE_SHRUNK)) {
 		sock_reset_flag(sk, SOCK_QUEUE_SHRUNK);
 		if (sk->sk_socket &&
@@ -3555,6 +3928,8 @@
 
 static inline void tcp_data_snd_check(struct sock *sk, struct tcp_sock *tp)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_data_snd_check [tcp_input.c]\n");
 	tcp_push_pending_frames(sk, tp);
 	tcp_check_space(sk);
 }
@@ -3566,6 +3941,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_input)
+        printk(" *** _tcp_ack_snd_check [tcp_input.c]\n");
+
 	    /* More than one full frame received... */
 	if (((tp->rcv_nxt - tp->rcv_wup) > inet_csk(sk)->icsk_ack.rcv_mss
 	     /* ... and right edge of window advances far enough.
@@ -3587,6 +3965,9 @@
 
 static inline void tcp_ack_snd_check(struct sock *sk)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_ack_snd_check [tcp_input.c]\n");
+
 	if (!inet_csk_ack_scheduled(sk)) {
 		/* We sent a data segment already. */
 		return;
@@ -3609,6 +3990,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	u32 ptr = ntohs(th->urg_ptr);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_check_urg [tcp_input.c]\n");
+
 	if (ptr && !sysctl_tcp_stdurg)
 		ptr--;
 	ptr += ntohl(th->seq);
@@ -3675,6 +4059,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_urg [tcp_input.c]\n");
+
 	/* Check if we get a new urgent pointer - normally not. */
 	if (th->urg)
 		tcp_check_urg(sk,th);
@@ -3702,6 +4089,9 @@
 	int chunk = skb->len - hlen;
 	int err;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_copy_to_iovec [tcp_input.c]\n");
+
 	local_bh_enable();
 	if (skb->ip_summed==CHECKSUM_UNNECESSARY)
 		err = skb_copy_datagram_iovec(skb, hlen, tp->ucopy.iov, chunk);
@@ -3723,6 +4113,9 @@
 {
 	int result;
 
+    if (debug_tcp_input)
+        printk(" *** __tcp_checksum_complete_user [tcp_input.c]\n");
+
 	if (sock_owned_by_user(sk)) {
 		local_bh_enable();
 		result = __tcp_checksum_complete(skb);
@@ -3735,6 +4128,8 @@
 
 static inline int tcp_checksum_complete_user(struct sock *sk, struct sk_buff *skb)
 {
+    if (debug_tcp_input)
+        printk(" *** tcp_checksum_complete_user [tcp_input.c]\n");
 	return skb->ip_summed != CHECKSUM_UNNECESSARY &&
 		__tcp_checksum_complete_user(sk, skb);
 }
@@ -3767,6 +4162,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_input)
+        printk(" *** tcp_rcv_established [tcp_input.c]\n");
+
 	/*
 	 *	Header prediction.
 	 *	The code loosely follows the one in the famous 
@@ -4010,6 +4408,9 @@
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	int saved_clamp = tp->rx_opt.mss_clamp;
 
+    if (debug_tcp_input)
+        printk(" *** tcp_rcv_synsent_state_process [tcp_input.c]\n");
+
 	tcp_parse_options(skb, &tp->rx_opt, 0);
 
 	if (th->ack) {
@@ -4118,6 +4519,13 @@
 
 		tcp_init_congestion_control(sk);
 
+        if (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr) {
+            char tcp_cong_name[TCP_CA_NAME_MAX];
+            tcp_get_default_congestion_control(tcp_cong_name);
+            if (strcmp(tcp_cong_name, "rcp") == 0) 
+                icsk->icsk_ca_ops->rtt_sample(sk, jiffies_to_usecs(tcp_time_stamp - tp->rx_opt.rcv_tsecr));
+        }
+
 		/* Prevent spurious tcp_cwnd_restart() on first data
 		 * packet.
 		 */
@@ -4261,6 +4669,9 @@
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	int queued = 0;
 
+    if (debug_tcp_input || debug_rtt)
+        printk(" *** tcp_rcv_state_process [tcp_input.c]\n");
+
 	tp->rx_opt.saw_tstamp = 0;
 
 	switch (sk->sk_state) {
@@ -4394,6 +4805,17 @@
 
 				tcp_init_congestion_control(sk);
 
+                if (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr) {
+
+                    char tcp_cong_name[TCP_CA_NAME_MAX];
+                    tcp_get_default_congestion_control(tcp_cong_name);
+
+                    if (strcmp(tcp_cong_name, "rcp") == 0) 
+                        icsk->icsk_ca_ops->rtt_sample(sk, jiffies_to_usecs(tcp_time_stamp - tp->rx_opt.rcv_tsecr));
+
+                    // printk(" Recd SYNACK: RTT = %u jiffies %u msecs %u usecs\n", tcp_time_stamp - tp->rx_opt.rcv_tsecr, jiffies_to_msecs(tcp_time_stamp - tp->rx_opt.rcv_tsecr), RTTusec);
+                }
+
 				/* Prevent spurious tcp_cwnd_restart() on
 				 * first data packet.
 				 */
Index: linux-2.6.16/net/ipv4/tcp_timer.c
===================================================================
--- linux-2.6.16/net/ipv4/tcp_timer.c	(revision 155)
+++ linux-2.6.16/net/ipv4/tcp_timer.c	(revision 300)
@@ -35,11 +35,19 @@
 static void tcp_write_timer(unsigned long);
 static void tcp_delack_timer(unsigned long);
 static void tcp_keepalive_timer (unsigned long data);
+static void rcp_packet_pacing_timer(unsigned long);
 
 void tcp_init_xmit_timers(struct sock *sk)
 {
 	inet_csk_init_xmit_timers(sk, &tcp_write_timer, &tcp_delack_timer,
 				  &tcp_keepalive_timer);
+
+    /* Nandita: Initialize RCP pacing timer */
+    init_timer(&(tcp_sk(sk)->rcp_pacing_timer));
+    tcp_sk(sk)->rcp_pacing_timer.function = &rcp_packet_pacing_timer;
+    tcp_sk(sk)->rcp_pacing_timer.data     = (unsigned long) sk;
+    /* Nandita: End */
+
 }
 
 EXPORT_SYMBOL(tcp_init_xmit_timers);
@@ -384,6 +392,76 @@
 out:;
 }
 
+/* Nandita: Timer used to pace packets. */
+
+static void rcp_packet_pacing_timer(unsigned long data) {
+
+    struct sock *sk = (struct sock*)data;
+    struct tcp_sock *tp = tcp_sk(sk);
+
+    printk(" ### rcp_packet_pacing_timer [tcp_timer.c]\n");
+
+    if (!tp->rcp_pacing) 
+        return;
+
+    bh_lock_sock(sk);
+    if (sock_owned_by_user(sk)) {
+        if (!mod_timer(&tp->rcp_pacing_timer, jiffies+1))
+            sock_hold(sk);
+
+        goto out_unlock;
+    }
+
+    if (sk->sk_state == TCP_CLOSE)
+        goto out_unlock;
+
+    /* Unlock sending */
+    tp->rcp_pacing_lock = 0;
+
+    if (!sk->sk_send_head) 
+        goto out_unlock;
+
+    tcp_push_pending_frames(sk, tp);
+
+out_unlock: 
+    bh_unlock_sock(sk);
+    sock_put(sk);
+  
+ return;
+}
+
+/* Nandita: Takes as input the RCP rate and the datagram size (incl. RCP
+ * header) and returns the pacing interval (in jiffies) 
+ * rcp_rate: Bytes/msec
+ * datagram_size: Bytes */
+
+unsigned int rcp_tcp_pacing_delta(unsigned int rcp_rate, unsigned int datagram_size) {
+
+    unsigned int pacing_delta_jiffies;
+
+    pacing_delta_jiffies = max(1, (unsigned int) msecs_to_jiffies(datagram_size/rcp_rate));
+
+    printk(" ### rcp_tcp_pacing_delta [tcp_timer.c] %u jiffies\n", pacing_delta_jiffies);
+    return (pacing_delta_jiffies);
+}
+
+/* Nandita: Resets pacing timer to fire at a new time */
+
+void rcp_tcp_reset_pacing_timer(struct sock *sk)
+{
+    struct tcp_sock *tp = tcp_sk(sk);
+
+    if (!tp->rcp_pacing)
+        return;
+
+    __u32 timeout = jiffies + tp->rcp_pacing_interval;
+
+    printk(" ### rcp_tcp_reset_pacing_timer [tcp_timer.c]\n");
+
+    if (!mod_timer(&tp->rcp_pacing_timer, timeout))
+        sock_hold(sk);
+}
+
 static void tcp_write_timer(unsigned long data)
 {
 	struct sock *sk = (struct sock*)data;
Index: linux-2.6.16/net/ipv4/tcp_ipv4.c
===================================================================
--- linux-2.6.16/net/ipv4/tcp_ipv4.c	(revision 155)
+++ linux-2.6.16/net/ipv4/tcp_ipv4.c	(revision 300)
@@ -69,6 +69,7 @@
 #include <net/transp_v6.h>
 #include <net/ipv6.h>
 #include <net/inet_common.h>
+#include <net/rcp.h>
 #include <net/timewait_sock.h>
 #include <net/xfrm.h>
 
@@ -78,12 +79,16 @@
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 
+#include <linux/rcp.h>
+
 int sysctl_tcp_tw_reuse;
 int sysctl_tcp_low_latency;
 
 /* Check TCP sequence numbers in ICMP packets. */
 #define ICMP_MIN_LENGTH 8
 
+#define debug_tcp_ipv4 0
+
 /* Socket used for sending RSTs */
 static struct socket *tcp_socket;
 
@@ -97,22 +102,30 @@
 
 static int tcp_v4_get_port(struct sock *sk, unsigned short snum)
 {
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_get_port() [tcp_ipv4.c]\n");
 	return inet_csk_get_port(&tcp_hashinfo, sk, snum,
 				 inet_csk_bind_conflict);
 }
 
 static void tcp_v4_hash(struct sock *sk)
 {
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_hash() [tcp_ipv4.c]\n");
 	inet_hash(&tcp_hashinfo, sk);
 }
 
 void tcp_unhash(struct sock *sk)
 {
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_unhash() [tcp_ipv4.c]\n");
 	inet_unhash(&tcp_hashinfo, sk);
 }
 
 static inline __u32 tcp_v4_init_sequence(struct sock *sk, struct sk_buff *skb)
 {
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_init_sequence() [tcp_ipv4.c]\n");
 	return secure_tcp_sequence_number(skb->nh.iph->daddr,
 					  skb->nh.iph->saddr,
 					  skb->h.th->dest,
@@ -124,6 +137,9 @@
 	const struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_twsk_unique() [tcp_ipv4.c]\n");
+
 	/* With PAWS, it is safe from the viewpoint
 	   of data integrity. Even without PAWS it is safe provided sequence
 	   spaces do not overlap i.e. at data rates <= 80Mbit/sec.
@@ -163,6 +179,9 @@
 	int tmp;
 	int err;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_connect() [tcp_ipv4.c]\n");
+
 	if (addr_len < sizeof(struct sockaddr_in))
 		return -EINVAL;
 
@@ -275,6 +294,9 @@
 	struct dst_entry *dst;
 	struct inet_sock *inet = inet_sk(sk);
 
+    if (debug_tcp_ipv4)
+        printk(" !!! do_pmtu_discovery() [tcp_ipv4.c]\n");
+
 	/* We are not interested in TCP_LISTEN and open_requests (SYN-ACKs
 	 * send out by Linux are always <576bytes so they should go through
 	 * unfragmented).
@@ -342,6 +364,9 @@
 	__u32 seq;
 	int err;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_err() [tcp_ipv4.c]\n");
+
 	if (skb->len < (iph->ihl << 2) + 8) {
 		ICMP_INC_STATS_BH(ICMP_MIB_INERRORS);
 		return;
@@ -484,6 +509,9 @@
 	struct inet_sock *inet = inet_sk(sk);
 	struct tcphdr *th = skb->h.th;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_send_check() [tcp_ipv4.c]\n");
+
 	if (skb->ip_summed == CHECKSUM_HW) {
 		th->check = ~tcp_v4_check(th, len, inet->saddr, inet->daddr, 0);
 		skb->csum = offsetof(struct tcphdr, check);
@@ -514,6 +542,9 @@
 	struct tcphdr rth;
 	struct ip_reply_arg arg;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_send_reset() [tcp_ipv4.c]\n");
+
 	/* Never send a reset in response to a reset. */
 	if (th->rst)
 		return;
@@ -564,6 +595,9 @@
 	} rep;
 	struct ip_reply_arg arg;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_send_ack() [tcp_ipv4.c]\n");
+
 	memset(&rep.th, 0, sizeof(struct tcphdr));
 	memset(&arg, 0, sizeof arg);
 
@@ -602,6 +636,9 @@
 	struct inet_timewait_sock *tw = inet_twsk(sk);
 	const struct tcp_timewait_sock *tcptw = tcp_twsk(sk);
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_timewait_ack() [tcp_ipv4.c]\n");
+
 	tcp_v4_send_ack(skb, tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,
 			tcptw->tw_rcv_wnd >> tw->tw_rcv_wscale, tcptw->tw_ts_recent);
 
@@ -610,6 +647,9 @@
 
 static void tcp_v4_reqsk_send_ack(struct sk_buff *skb, struct request_sock *req)
 {
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_reqsk_send_ack() [tcp_ipv4.c]\n");
+
 	tcp_v4_send_ack(skb, tcp_rsk(req)->snt_isn + 1, tcp_rsk(req)->rcv_isn + 1, req->rcv_wnd,
 			req->ts_recent);
 }
@@ -626,6 +666,9 @@
 	int err = -1;
 	struct sk_buff * skb;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_send_synack() [tcp_ipv4.c]\n");
+
 	/* First, grab a route. */
 	if (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)
 		goto out;
@@ -641,9 +684,14 @@
 					 csum_partial((char *)th, skb->len,
 						      skb->csum));
 
+        /*
 		err = ip_build_and_send_pkt(skb, sk, ireq->loc_addr,
 					    ireq->rmt_addr,
 					    ireq->opt);
+        */
+
+        err = rcp_build_and_send_pkt(skb, sk, ireq->loc_addr, ireq->rmt_addr, ireq->opt);
+        
 		if (err == NET_XMIT_CN)
 			err = 0;
 	}
@@ -658,6 +706,8 @@
  */
 static void tcp_v4_reqsk_destructor(struct request_sock *req)
 {
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_reqsk_destructor() [tcp_ipv4.c]\n");
 	kfree(inet_rsk(req)->opt);
 }
 
@@ -666,6 +716,9 @@
 {
 	static unsigned long warntime;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! syn_flood_warning() [tcp_ipv4.c]\n");
+
 	if (time_after(jiffies, (warntime + HZ * 60))) {
 		warntime = jiffies;
 		printk(KERN_INFO
@@ -684,6 +737,9 @@
 	struct ip_options *opt = &(IPCB(skb)->opt);
 	struct ip_options *dopt = NULL;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_save_options() [tcp_ipv4.c]\n");
+
 	if (opt && opt->optlen) {
 		int opt_size = optlength(opt);
 		dopt = kmalloc(opt_size, GFP_ATOMIC);
@@ -726,6 +782,9 @@
 #define want_cookie 0 /* Argh, why doesn't gcc optimize this :( */
 #endif
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_conn_request() [tcp_ipv4.c]\n");
+
 	/* Never answer to SYNs send to broadcast or multicast */
 	if (((struct rtable *)skb->dst)->rt_flags &
 	    (RTCF_BROADCAST | RTCF_MULTICAST))
@@ -873,6 +932,9 @@
 	struct tcp_sock *newtp;
 	struct sock *newsk;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_syn_recv_sock() [tcp_ipv4.c]\n");
+
 	if (sk_acceptq_is_full(sk))
 		goto exit_overflow;
 
@@ -926,6 +988,9 @@
 	/* Find possible connection requests. */
 	struct request_sock *req = inet_csk_search_req(sk, &prev, th->source,
 						       iph->saddr, iph->daddr);
+
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_hnd_req() [tcp_ipv4.c]\n");
 	if (req)
 		return tcp_check_req(sk, skb, req, prev);
 
@@ -951,6 +1016,9 @@
 
 static int tcp_v4_checksum_init(struct sk_buff *skb)
 {
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_checksum_init() [tcp_ipv4.c]\n");
+
 	if (skb->ip_summed == CHECKSUM_HW) {
 		if (!tcp_v4_check(skb->h.th, skb->len, skb->nh.iph->saddr,
 				  skb->nh.iph->daddr, skb->csum)) {
@@ -979,6 +1047,41 @@
  */
 int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
 {
+    struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_do_rcv() [tcp_ipv4.c]\n");
+
+    /* Nandita: 
+     * 1. Does this packet have RCP on IP ?
+     * 2. YES: Read RCP values from header into tcp_sock
+     * 3. NO: not using RCP, too bad. 
+     */
+
+    if (skb->nh.iph->protocol == IPPROTO_RCP) {
+        struct rcphdr *rcph = (struct rcphdr*)(skb->data - sizeof(struct rcphdr));
+        char tcp_cong_name[TCP_CA_NAME_MAX];
+        tcp_get_default_congestion_control(tcp_cong_name);
+        
+        printk(" tcp_v4_do_rcv():\n");
+        printk(" tcp_v4_do_rcv(): tcp_cong_name %s tp->tcp_header_len %u sizeof(struct rcphdr) %u skb->nh.iph->ihl*4 %u \n", tcp_cong_name, tp->tcp_header_len, sizeof(struct rcphdr), skb->nh.iph->ihl*4);
+
+        if (ntohl(rcph->rcp_reverse_bottleneck_rate) > 0)
+            tp->rcp_bottleneck_rate = ntohl(rcph->rcp_reverse_bottleneck_rate);
+
+        if (ntohl(rcph->rcp_bottleneck_rate) > 0)
+            tp->rcp_reverse_bottleneck_rate = ntohl(rcph->rcp_bottleneck_rate);
+
+        if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) {
+            unsigned int datagram_size = tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE;
+            tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(datagram_size, 1));
+            tp->rcp_pacing_interval = rcp_tcp_pacing_delta(tp->rcp_bottleneck_rate, datagram_size); 
+        }
+
+        printk("tcp_v4_do_rcv(): snd_cwnd %u rcp_fwd_rate %u rcp_rtt %u mss %u \n", tp->snd_cwnd, tp->rcp_bottleneck_rate, tp->rcp_rtt, tp->mss_cache);
+    }
+
+    
 	if (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */
 		TCP_CHECK_TIMER(sk);
 		if (tcp_rcv_established(sk, skb, skb->h.th, skb->len))
@@ -1034,6 +1137,9 @@
 	struct sock *sk;
 	int ret;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_rcv() [tcp_ipv4.c]\n");
+
 	if (skb->pkt_type != PACKET_HOST)
 		goto discard_it;
 
@@ -1171,6 +1277,9 @@
 	struct inet_peer *peer = NULL;
 	int release_it = 0;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_remember_stamp() [tcp_ipv4.c]\n");
+
 	if (!rt || rt->rt_dst != inet->daddr) {
 		peer = inet_getpeer(inet->daddr, 1);
 		release_it = 1;
@@ -1199,6 +1308,9 @@
 {
 	struct inet_peer *peer = inet_getpeer(tw->tw_daddr, 1);
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_tw_remember_stamp() [tcp_ipv4.c]\n");
+
 	if (peer) {
 		const struct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);
 
@@ -1216,7 +1328,10 @@
 }
 
 struct inet_connection_sock_af_ops ipv4_specific = {
+    /*
 	.queue_xmit	=	ip_queue_xmit,
+    */
+    .queue_xmit =   rcp_queue_xmit,
 	.send_check	=	tcp_v4_send_check,
 	.rebuild_header	=	inet_sk_rebuild_header,
 	.conn_request	=	tcp_v4_conn_request,
@@ -1237,6 +1352,9 @@
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_init_sock() [tcp_ipv4.c]\n");
+
 	skb_queue_head_init(&tp->out_of_order_queue);
 	tcp_init_xmit_timers(sk);
 	tcp_prequeue_init(tp);
@@ -1272,6 +1390,19 @@
 	sk->sk_sndbuf = sysctl_tcp_wmem[1];
 	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
 
+    /* Nandita: Initialize RCP stuff */
+    tp->rcp_bottleneck_rate               = 0;
+    tp->rcp_reverse_bottleneck_rate       = 0;
+    tp->rcp_rtt                           = 0;
+    tp->rcp_minRTT                        = ~0;
+    tp->rcp_sRTT                          = 0; 
+
+    tp->rcp_pacing_interval               = 0;
+    tp->rcp_last_packet_transmitted_time  = 0;
+    tp->rcp_pacing                        = 0;
+    tp->rcp_pacing_lock                   = 0;
+    /* Nandita: End */
+
 	atomic_inc(&tcp_sockets_allocated);
 
 	return 0;
@@ -1281,6 +1412,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_destroy_sock() [tcp_ipv4.c]\n");
+
 	tcp_clear_xmit_timers(sk);
 
 	tcp_cleanup_congestion_control(sk);
@@ -1688,6 +1822,9 @@
 	__u16 destp = ntohs(inet->dport);
 	__u16 srcp = ntohs(inet->sport);
 
+    if (debug_tcp_ipv4)
+        printk(" !!! get_tcp4_sock() [tcp_ipv4.c]\n");
+
 	if (icsk->icsk_pending == ICSK_TIME_RETRANS) {
 		timer_active	= 1;
 		timer_expires	= icsk->icsk_timeout;
@@ -1726,6 +1863,9 @@
 	__u16 destp, srcp;
 	int ttd = tw->tw_ttd - jiffies;
 
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_timewait4_sock() [tcp_ipv4.c]\n");
+
 	if (ttd < 0)
 		ttd = 0;
 
@@ -1832,6 +1972,9 @@
 void __init tcp_v4_init(struct net_proto_family *ops)
 {
 	int err = sock_create_kern(PF_INET, SOCK_RAW, IPPROTO_TCP, &tcp_socket);
+
+    if (debug_tcp_ipv4)
+        printk(" !!! tcp_v4_init() [tcp_ipv4.c]\n");
 	if (err < 0)
 		panic("Failed to create the TCP control socket.\n");
 	tcp_socket->sk->sk_allocation   = GFP_ATOMIC;
Index: linux-2.6.16/net/ipv4/Makefile
===================================================================
--- linux-2.6.16/net/ipv4/Makefile	(revision 155)
+++ linux-2.6.16/net/ipv4/Makefile	(revision 300)
@@ -9,6 +9,7 @@
 	     tcp.o tcp_input.o tcp_output.o tcp_timer.o tcp_ipv4.o \
 	     tcp_minisocks.o tcp_cong.o \
 	     datagram.o raw.o udp.o arp.o icmp.o devinet.o af_inet.o igmp.o \
+         rcp.o tcp_rcp.o \
 	     sysctl_net_ipv4.o fib_frontend.o fib_semantics.o
 
 obj-$(CONFIG_IP_FIB_HASH) += fib_hash.o
Index: linux-2.6.16/net/ipv4/af_inet.c
===================================================================
--- linux-2.6.16/net/ipv4/af_inet.c	(revision 155)
+++ linux-2.6.16/net/ipv4/af_inet.c	(revision 300)
@@ -108,6 +108,7 @@
 #include <net/sock.h>
 #include <net/raw.h>
 #include <net/icmp.h>
+#include <net/rcp.h>
 #include <net/ipip.h>
 #include <net/inet_common.h>
 #include <net/xfrm.h>
@@ -1106,6 +1107,12 @@
 	.handler =	icmp_rcv,
 };
 
+/* Nandita: Start */
+static struct net_protocol rcp_protocol = {
+        .handler =  rcp_v4_rcv,
+};
+/* Nandita: End */
+
 static int __init init_ipv4_mibs(void)
 {
 	net_statistics[0] = alloc_percpu(struct linux_mib);
@@ -1174,6 +1181,12 @@
 	 *	Add all the base protocols.
 	 */
 
+    /* Nandita: Start */
+    if (inet_add_protocol(&rcp_protocol, IPPROTO_RCP) < 0)
+       printk(KERN_CRIT "inet_init: Cannot add RCP protocol\n");
+    /* Nandita: End */ 
+    printk(" *** inet_init [af_inet.c]: Added RCP protocol\n");
+
 	if (inet_add_protocol(&icmp_protocol, IPPROTO_ICMP) < 0)
 		printk(KERN_CRIT "inet_init: Cannot add ICMP protocol\n");
 	if (inet_add_protocol(&udp_protocol, IPPROTO_UDP) < 0)
Index: linux-2.6.16/net/ipv4/tcp_output.c
===================================================================
--- linux-2.6.16/net/ipv4/tcp_output.c	(revision 155)
+++ linux-2.6.16/net/ipv4/tcp_output.c	(revision 300)
@@ -42,6 +42,10 @@
 #include <linux/module.h>
 #include <linux/smp_lock.h>
 
+#include <linux/rcp.h>
+
+#define debug_tcp_output 0
+
 /* People can turn this off for buggy TCP's found in printers etc. */
 int sysctl_tcp_retrans_collapse = 1;
 
@@ -54,6 +58,9 @@
 static void update_send_head(struct sock *sk, struct tcp_sock *tp,
 			     struct sk_buff *skb)
 {
+    if (debug_tcp_output)
+        printk(" +++ update_send_head [tcp_output.c]\n");
+
 	sk->sk_send_head = skb->next;
 	if (sk->sk_send_head == (struct sk_buff *)&sk->sk_write_queue)
 		sk->sk_send_head = NULL;
@@ -69,6 +76,9 @@
  */
 static inline __u32 tcp_acceptable_seq(struct sock *sk, struct tcp_sock *tp)
 {
+    if (debug_tcp_output)
+        printk(" +++ tcp_acceptable_seq [tcp_output.c]\n");
+
 	if (!before(tp->snd_una+tp->snd_wnd, tp->snd_nxt))
 		return tp->snd_nxt;
 	else
@@ -95,6 +105,9 @@
 	struct dst_entry *dst = __sk_dst_get(sk);
 	int mss = tp->advmss;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_advertise_mss [tcp_output.c]\n");
+
 	if (dst && dst_metric(dst, RTAX_ADVMSS) < mss) {
 		mss = dst_metric(dst, RTAX_ADVMSS);
 		tp->advmss = mss;
@@ -105,6 +118,7 @@
 
 /* RFC2861. Reset CWND after idle period longer RTO to "restart window".
  * This is the first part of cwnd validation mechanism. */
+
 static void tcp_cwnd_restart(struct sock *sk, struct dst_entry *dst)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -112,6 +126,12 @@
 	u32 restart_cwnd = tcp_init_cwnd(tp, dst);
 	u32 cwnd = tp->snd_cwnd;
 
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_output)
+        printk(" +++ tcp_cwnd_restart [tcp_output.c]\n");
+
 	tcp_ca_event(sk, CA_EVENT_CWND_RESTART);
 
 	tp->snd_ssthresh = tcp_current_ssthresh(sk);
@@ -119,7 +139,15 @@
 
 	while ((delta -= inet_csk(sk)->icsk_rto) > 0 && cwnd > restart_cwnd)
 		cwnd >>= 1;
-	tp->snd_cwnd = max(cwnd, restart_cwnd);
+
+    /* Nandita: tcp_rcp sends out probe packets in every RTT to get the most
+     * recent rate when the application is idle. */
+    
+    if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+    else 
+        tp->snd_cwnd = max(cwnd, restart_cwnd);
+
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 	tp->snd_cwnd_used = 0;
 }
@@ -130,6 +158,9 @@
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	const u32 now = tcp_time_stamp;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_event_data_sent [tcp_output.c]\n");
+
 	if (!tp->packets_out && (s32)(now - tp->lsndtime) > icsk->icsk_rto)
 		tcp_cwnd_restart(sk, __sk_dst_get(sk));
 
@@ -144,6 +175,9 @@
 
 static inline void tcp_event_ack_sent(struct sock *sk, unsigned int pkts)
 {
+    if (debug_tcp_output)
+        printk(" +++ tcp_event_ack_sent [tcp_output.c]\n");
+
 	tcp_dec_quickack_mode(sk, pkts);
 	inet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);
 }
@@ -161,6 +195,9 @@
 {
 	unsigned int space = (__space < 0 ? 0 : __space);
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_select_initial_window [tcp_output.c]\n");
+
 	/* If no clamp set the clamp to the max possible scaled window */
 	if (*window_clamp == 0)
 		(*window_clamp) = (65535 << 14);
@@ -218,6 +255,9 @@
 	u32 cur_win = tcp_receive_window(tp);
 	u32 new_win = __tcp_select_window(sk);
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_select_window [tcp_output.c]\n");
+
 	/* Never shrink the offered window */
 	if(new_win < cur_win) {
 		/* Danger Will Robinson!
@@ -253,6 +293,10 @@
 static void tcp_build_and_update_options(__u32 *ptr, struct tcp_sock *tp,
 					 __u32 tstamp)
 {
+
+    if (debug_tcp_output)
+        printk(" +++ tcp_build_and_update_options [tcp_output.c]\n");
+
 	if (tp->rx_opt.tstamp_ok) {
 		*ptr++ = __constant_htonl((TCPOPT_NOP << 24) |
 					  (TCPOPT_NOP << 16) |
@@ -303,6 +347,9 @@
 	 * SACKs don't matter, we never delay an ACK when we
 	 * have any of those going out.
 	 */
+    if (debug_tcp_output)
+        printk(" +++ tcp_syn_build_options [tcp_output.c]\n");
+
 	*ptr++ = htonl((TCPOPT_MSS << 24) | (TCPOLEN_MSS << 16) | mss);
 	if (ts) {
 		if(sack)
@@ -342,6 +389,9 @@
 	int sysctl_flags;
 	int err;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_transmit_skb [tcp_output.c]\n");
+
 	BUG_ON(!skb || !tcp_skb_pcount(skb));
 
 	/* If congestion control is doing timestamping, we must
@@ -478,6 +528,8 @@
 static void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+    if (debug_tcp_output)
+        printk(" +++ tcp_queue_skb [tcp_output.c]\n");
 
 	/* Advance write_seq and place onto the write_queue. */
 	tp->write_seq = TCP_SKB_CB(skb)->end_seq;
@@ -492,6 +544,9 @@
 
 static void tcp_set_skb_tso_segs(struct sock *sk, struct sk_buff *skb, unsigned int mss_now)
 {
+    if (debug_tcp_output)
+        printk(" +++ tcp_set_skb_tso_segs [tcp_output.c]\n");
+
 	if (skb->len <= mss_now ||
 	    !(sk->sk_route_caps & NETIF_F_TSO)) {
 		/* Avoid the costly divide in the normal
@@ -521,6 +576,9 @@
 	int nsize, old_factor;
 	u16 flags;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_fragment [tcp_output.c]\n");
+
 	BUG_ON(len > skb->len);
 
  	clear_all_retrans_hints(tp);
@@ -627,6 +685,9 @@
 {
 	int i, k, eat;
 
+    if (debug_tcp_output)
+        printk(" +++ __pskb_trim_head [tcp_output.c]\n");
+
 	eat = len;
 	k = 0;
 	for (i=0; i<skb_shinfo(skb)->nr_frags; i++) {
@@ -653,6 +714,9 @@
 
 int tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)
 {
+    if (debug_tcp_output)
+        printk(" +++ tcp_trim_head [tcp_output.c]\n");
+
 	if (skb_cloned(skb) &&
 	    pskb_expand_head(skb, 0, 0, GFP_ATOMIC))
 		return -ENOMEM;
@@ -711,9 +775,14 @@
 	/* Calculate base mss without TCP options:
 	   It is MMS_S - sizeof(tcphdr) of rfc1122
 	 */
-	int mss_now = (pmtu - icsk->icsk_af_ops->net_header_len -
-		       sizeof(struct tcphdr));
+    /* Nandita: Before RCP */
+	// int mss_now = (pmtu - icsk->icsk_af_ops->net_header_len - sizeof(struct tcphdr));
+   
+    int mss_now = (pmtu - icsk->icsk_af_ops->net_header_len - sizeof(struct tcphdr) - sizeof(struct rcphdr));
 
+    //    if (debug_tcp_output)
+         printk(" +++ tcp_sync_mss [tcp_output.c]\n");
+
 	/* Clamp it (mss_clamp does not include tcp options) */
 	if (mss_now > tp->rx_opt.mss_clamp)
 		mss_now = tp->rx_opt.mss_clamp;
@@ -736,6 +805,8 @@
 	icsk->icsk_pmtu_cookie = pmtu;
 	tp->mss_cache = mss_now;
 
+    printk("tp->mss_cache %u, tp->rx_opt.mss_clamp %u ,icsk->icsk_ext_hdr_len %u, sizeof(struct tcphdr) %u, pmtu  %u\n", tp->mss_cache, tp->rx_opt.mss_clamp, icsk->icsk_ext_hdr_len,  sizeof(struct tcphdr), pmtu);
+
 	return mss_now;
 }
 
@@ -754,6 +825,9 @@
 	u16 xmit_size_goal;
 	int doing_tso = 0;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_current_mss [tcp_output.c]\n");
+
 	mss_now = tp->mss_cache;
 
 	if (large_allowed &&
@@ -795,6 +869,10 @@
 
 static void tcp_cwnd_validate(struct sock *sk, struct tcp_sock *tp)
 {
+
+    if (debug_tcp_output)
+        printk(" +++ tcp_cwnd_validation [tcp_output.c]\n");
+
 	__u32 packets_out = tp->packets_out;
 
 	if (packets_out >= tp->snd_cwnd) {
@@ -815,6 +893,9 @@
 {
 	u32 window, cwnd_len;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_window_allows [tcp_output.c]\n");
+
 	window = (tp->snd_una + tp->snd_wnd - TCP_SKB_CB(skb)->seq);
 	cwnd_len = mss_now * cwnd;
 	return min(window, cwnd_len);
@@ -827,6 +908,9 @@
 {
 	u32 in_flight, cwnd;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_cwnd_test [tcp_output.c]\n");
+
 	/* Don't be strict about the congestion window for the final FIN.  */
 	if (TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN)
 		return 1;
@@ -846,6 +930,9 @@
 {
 	int tso_segs = tcp_skb_pcount(skb);
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_init_tso_segs [tcp_output.c]\n");
+
 	if (!tso_segs ||
 	    (tso_segs > 1 &&
 	     skb_shinfo(skb)->tso_size != mss_now)) {
@@ -857,6 +944,8 @@
 
 static inline int tcp_minshall_check(const struct tcp_sock *tp)
 {
+    if (debug_tcp_output)
+        printk(" +++ tcp_minshall_check [tcp_output.c]\n");
 	return after(tp->snd_sml,tp->snd_una) &&
 		!after(tp->snd_sml, tp->snd_nxt);
 }
@@ -873,6 +962,8 @@
 				  const struct sk_buff *skb, 
 				  unsigned mss_now, int nonagle)
 {
+    if (debug_tcp_output)
+        printk(" +++ tcp_nagle_check [tcp_output.c]\n");
 	return (skb->len < mss_now &&
 		((nonagle&TCP_NAGLE_CORK) ||
 		 (!nonagle &&
@@ -892,6 +983,8 @@
 	 * This is implemented in the callers, where they modify the 'nonagle'
 	 * argument based upon the location of SKB in the send queue.
 	 */
+    if (debug_tcp_output)
+        printk(" +++ tcp_nagle_test [tcp_output.c]\n");
 	if (nonagle & TCP_NAGLE_PUSH)
 		return 1;
 
@@ -911,6 +1004,9 @@
 {
 	u32 end_seq = TCP_SKB_CB(skb)->end_seq;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_snd_wnd_test [tcp_output.c]\n");
+
 	if (skb->len > cur_mss)
 		end_seq = TCP_SKB_CB(skb)->seq + cur_mss;
 
@@ -927,6 +1023,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	unsigned int cwnd_quota;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_snd_test [tcp_output.c]\n");
+
 	tcp_init_tso_segs(sk, skb, cur_mss);
 
 	if (!tcp_nagle_test(tp, skb, cur_mss, nonagle))
@@ -943,6 +1042,8 @@
 static inline int tcp_skb_is_last(const struct sock *sk, 
 				  const struct sk_buff *skb)
 {
+    if (debug_tcp_output)
+        printk(" +++ tcp_skb_is_last [tcp_output.c]\n");
 	return skb->next == (struct sk_buff *)&sk->sk_write_queue;
 }
 
@@ -950,6 +1051,9 @@
 {
 	struct sk_buff *skb = sk->sk_send_head;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_may_send_now [tcp_output.c]\n");
+
 	return (skb &&
 		tcp_snd_test(sk, skb, tcp_current_mss(sk, 1),
 			     (tcp_skb_is_last(sk, skb) ?
@@ -970,6 +1074,9 @@
 	int nlen = skb->len - len;
 	u16 flags;
 
+    if (debug_tcp_output)
+        printk(" +++ tso_fragment [tcp_output.c]\n");
+
 	/* All of a TSO frame must be composed of paged data.  */
 	if (skb->len != skb->data_len)
 		return tcp_fragment(sk, skb, len, mss_now);
@@ -1018,6 +1125,9 @@
 	const struct inet_connection_sock *icsk = inet_csk(sk);
 	u32 send_win, cong_win, limit, in_flight;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_tso_should_defer [tcp_output.c]\n");
+
 	if (TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN)
 		return 0;
 
@@ -1077,6 +1187,13 @@
 	unsigned int tso_segs, sent_pkts;
 	int cwnd_quota;
 
+    /* Nandita: For pacing purposes */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+
+    if (debug_tcp_output)
+        printk(" +++ tcp_write_xmit [tcp_output.c]\n");
+
 	/* If we are closed, the bytes will have to remain here.
 	 * In time closedown will finish, we empty the write queue and all
 	 * will be happy.
@@ -1088,6 +1205,11 @@
 	while ((skb = sk->sk_send_head)) {
 		unsigned int limit;
 
+        /* Nandita: If you are using RCP congestion-control and pacing is
+         * locked then get out */
+        if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_pacing) && (tp->rcp_pacing_interval > 0) && (tp->rcp_pacing_lock == 1)) 
+            break;
+
 		tso_segs = tcp_init_tso_segs(sk, skb, mss_now);
 		BUG_ON(!tso_segs);
 
@@ -1137,6 +1259,17 @@
 
 		tcp_minshall_update(tp, mss_now, skb);
 		sent_pkts++;
+         
+        /* Nandita: 1. Update when the last segment was sent 
+                    2. Reset the pacing timer */
+
+        if ( (strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_pacing) && (tp->rcp_pacing_interval > 0)) {
+            printk(" ### tcp_write_xmit() Reset Pacing Timer [tcp_output.c]\n");
+            tp->rcp_last_packet_transmitted_time = tcp_time_stamp;  
+            tp->rcp_pacing_lock = 1;
+            rcp_tcp_reset_pacing_timer(sk);
+        }
+
 	}
 
 	if (likely(sent_pkts)) {
@@ -1155,6 +1288,9 @@
 {
 	struct sk_buff *skb = sk->sk_send_head;
 
+    if (debug_tcp_output)
+        printk(" +++ __tcp_push_pending_frames [tcp_output.c]\n");
+
 	if (skb) {
 		if (tcp_write_xmit(sk, cur_mss, nonagle))
 			tcp_check_probe_timer(sk, tp);
@@ -1170,6 +1306,9 @@
 	struct sk_buff *skb = sk->sk_send_head;
 	unsigned int tso_segs, cwnd_quota;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_push_one [tcp_output.c]\n");
+
 	BUG_ON(!skb || skb->len < mss_now);
 
 	tso_segs = tcp_init_tso_segs(sk, skb, mss_now);
@@ -1275,6 +1414,9 @@
 	int full_space = min_t(int, tp->window_clamp, tcp_full_space(sk));
 	int window;
 
+    if (debug_tcp_output)
+        printk(" +++ __tcp_select_window [tcp_output.c]\n");
+
 	if (mss > full_space)
 		mss = full_space; 
 
@@ -1327,6 +1469,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *next_skb = skb->next;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_retrans_try_collapse [tcp_output.c]\n");
+
 	/* The first test we must make is that neither of these two
 	 * SKB's are still referenced by someone else.
 	 */
@@ -1411,6 +1556,9 @@
 	unsigned int mss = tcp_current_mss(sk, 0);
 	int lost = 0;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_simple_retransmit [tcp_output.c]\n");
+
 	sk_stream_for_retrans_queue(skb, sk) {
 		if (skb->len > mss && 
 		    !(TCP_SKB_CB(skb)->sacked&TCPCB_SACKED_ACKED)) {
@@ -1458,6 +1606,9 @@
  	unsigned int cur_mss = tcp_current_mss(sk, 0);
 	int err;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_retransmit_skb [tcp_output.c]\n");
+
 	/* Do not sent more than we queued. 1/4 is reserved for possible
 	 * copying overhead: fragmentation, tunneling, mangling etc.
 	 */
@@ -1566,6 +1717,9 @@
 	struct sk_buff *skb;
 	int packet_cnt;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_xmit_retransmit_queue [tcp_output.c]\n");
+
 	if (tp->retransmit_skb_hint) {
 		skb = tp->retransmit_skb_hint;
 		packet_cnt = tp->retransmit_cnt_hint;
@@ -1689,6 +1843,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);	
 	struct sk_buff *skb = skb_peek_tail(&sk->sk_write_queue);
 	int mss_now;
+
+    if (debug_tcp_output)
+        printk(" +++ tcp_send_fin [tcp_output.c]\n");
 	
 	/* Optimization, tack on the FIN if we have a queue of
 	 * unsent frames.  But be careful about outgoing SACKS
@@ -1735,6 +1892,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *skb;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_send_active_reset [tcp_output.c]\n");
+
 	/* NOTE: No TCP options attached and we never retransmit this. */
 	skb = alloc_skb(MAX_TCP_HEADER, priority);
 	if (!skb) {
@@ -1767,6 +1927,9 @@
 {
 	struct sk_buff* skb;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_send_synack [tcp_output.c]\n");
+
 	skb = skb_peek(&sk->sk_write_queue);
 	if (skb == NULL || !(TCP_SKB_CB(skb)->flags&TCPCB_FLAG_SYN)) {
 		printk(KERN_DEBUG "tcp_send_synack: wrong queue state\n");
@@ -1804,6 +1967,9 @@
 	int tcp_header_size;
 	struct sk_buff *skb;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_make_synack [tcp_output.c]\n");
+
 	skb = sock_wmalloc(sk, MAX_TCP_HEADER + 15, 1, GFP_ATOMIC);
 	if (skb == NULL)
 		return NULL;
@@ -1873,6 +2039,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	__u8 rcv_wscale;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_connect_init [tcp_output.c]\n");
+
 	/* We'll fix this up when we get a response from the other end.
 	 * See tcp_input.c:tcp_rcv_state_process case TCP_SYN_SENT.
 	 */
@@ -1923,6 +2092,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *buff;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_connect [tcp_output.c]\n");
+
 	tcp_connect_init(sk);
 
 	buff = alloc_skb_fclone(MAX_TCP_HEADER + 15, sk->sk_allocation);
@@ -1969,6 +2141,9 @@
 	int ato = icsk->icsk_ack.ato;
 	unsigned long timeout;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_send_delayed_ack [tcp_output.c]\n");
+
 	if (ato > TCP_DELACK_MIN) {
 		const struct tcp_sock *tp = tcp_sk(sk);
 		int max_ato = HZ/2;
@@ -2017,6 +2192,9 @@
 /* This routine sends an ack and also updates the window. */
 void tcp_send_ack(struct sock *sk)
 {
+
+    if (debug_tcp_output)
+        printk(" +++ tcp_send_ack [tcp_output.c]\n");
 	/* If we have been reset, we may not send again. */
 	if (sk->sk_state != TCP_CLOSE) {
 		struct tcp_sock *tp = tcp_sk(sk);
@@ -2066,6 +2244,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *skb;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_xmit_probe_skb [tcp_output.c]\n");
+
 	/* We don't queue it, tcp_transmit_skb() sets ownership. */
 	skb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);
 	if (skb == NULL) 
@@ -2091,6 +2272,10 @@
 
 int tcp_write_wakeup(struct sock *sk)
 {
+
+    if (debug_tcp_output)
+        printk(" +++ tcp_write_wakeup [tcp_output.c]\n");
+
 	if (sk->sk_state != TCP_CLOSE) {
 		struct tcp_sock *tp = tcp_sk(sk);
 		struct sk_buff *skb;
@@ -2143,6 +2328,9 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	int err;
 
+    if (debug_tcp_output)
+        printk(" +++ tcp_send_probe0 [tcp_output.c]\n");
+
 	err = tcp_write_wakeup(sk);
 
 	if (tp->packets_out || !sk->sk_send_head) {
Index: linux-2.6.16/net/ipv4/rcp.c
===================================================================
--- linux-2.6.16/net/ipv4/rcp.c	(revision 0)
+++ linux-2.6.16/net/ipv4/rcp.c	(revision 300)
@@ -0,0 +1,118 @@
+/*
+ *	NET3: Implementation of the RCP protocol layer. 
+ *	
+ *		Nandita Dukkipati, <nanditad@stanford.edu>
+ *
+ *	Version: $Id: rcp.c,v 1.0.0 07/11/2006 $
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	as published by the Free Software Foundation; either version
+ *	2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/config.h>
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/fcntl.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/inet.h>
+#include <linux/netdevice.h>
+#include <linux/string.h>
+#include <linux/netfilter_ipv4.h>
+#include <net/snmp.h>
+#include <net/ip.h>
+#include <net/route.h>
+#include <net/protocol.h>
+#include <net/rcp.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/raw.h>
+#include <linux/skbuff.h>
+#include <net/sock.h>
+#include <linux/errno.h>
+#include <linux/timer.h>
+#include <linux/init.h>
+#include <linux/rcp.h> 
+#include <asm/system.h>
+#include <asm/uaccess.h>
+#include <net/checksum.h>
+
+/* 
+ *	Deal with incoming RCP packets.
+ */
+ 
+int rcp_v4_rcv(struct sk_buff *skb)
+{
+    struct rcphdr *rcph = skb->data;
+    int rcphl = sizeof(struct rcphdr);
+    int i;
+
+    printk(" $$$ rcp_v4_rcv():\n");
+    printk(" $$$  rcp_bottleneck_rate %u \n", ntohl(rcph->rcp_bottleneck_rate));
+    printk(" $$$  rcp_reverse_bottleneck_rate %u \n", ntohl(rcph->rcp_reverse_bottleneck_rate));
+    printk(" $$$  rcp_rtt %u \n", ntohs(rcph->rcp_rtt));
+    printk(" $$$  rcp_p %u \n", rcph->protocol);
+
+    __skb_pull(skb, rcphl);
+    skb->h.raw = skb->data;
+
+    return(tcp_v4_rcv(skb));
+}
+
+/* 
+ * Called by transport layer to hand over packets to RCP 
+ */ 
+
+int rcp_queue_xmit(struct sk_buff *skb, int ipfragok)
+{
+
+    struct rcphdr *rcph;
+    int i;
+    unsigned char *ptr;
+    struct tcp_sock *tp = tcp_sk(skb->sk);
+
+    printk(" $$$ rcp_queue_xmit():\n");
+
+    rcph = (struct rcphdr *) skb_push(skb, sizeof(struct rcphdr));
+
+    rcph->rcp_bottleneck_rate = htonl(7500);
+    rcph->rcp_reverse_bottleneck_rate = htonl(tp->rcp_reverse_bottleneck_rate);
+    rcph->rcp_rtt = htons(tp->rcp_rtt);
+    rcph->protocol = IPPROTO_TCP;
+
+    return(ip_queue_xmit(skb, ipfragok));
+}
+
+void rcp_send_reply(struct sock *sk, struct sk_buff *skb, struct ip_reply_arg *arg, unsigned int len)
+{
+
+    /* Don't push RCP header. ip_build_xmit(.) will do skb_put for the entire
+     * segment */
+    printk(" $$$ rcp_send_reply():\n");
+    ip_send_reply(sk, skb, arg, len);
+
+//    ip_send_reply(sk, skb, arg, len+sizeof(struct rcphdr));
+}
+
+int rcp_build_and_send_pkt(struct sk_buff *skb, struct sock *sk, u32 saddr, u32 daddr, struct ip_options *opt)
+{
+    struct rcphdr *rcph;
+    int i;
+    unsigned char *ptr;
+    struct tcp_sock *tp = tcp_sk(sk);
+
+    printk(" $$$ rcp_build_and_send_pkt():\n");
+
+    rcph = (struct rcphdr *) skb_push(skb, sizeof(struct rcphdr));
+
+    rcph->rcp_bottleneck_rate = htonl(7500);
+    rcph->rcp_reverse_bottleneck_rate = htonl(tp->rcp_reverse_bottleneck_rate);
+    rcph->rcp_rtt = htons(tp->rcp_rtt);
+    rcph->protocol = IPPROTO_TCP;
+
+    return(ip_build_and_send_pkt(skb, sk, saddr, daddr, opt));
+}
Index: linux-2.6.16/net/ipv4/tcp.c
===================================================================
--- linux-2.6.16/net/ipv4/tcp.c	(revision 155)
+++ linux-2.6.16/net/ipv4/tcp.c	(revision 300)
@@ -1675,6 +1675,14 @@
 	inet_csk_delack_init(sk);
 	sk->sk_send_head = NULL;
 	tp->rx_opt.saw_tstamp = 0;
+
+    /* Nandita: RCP stuff */
+    tp->rcp_bottleneck_rate          = 0;
+    tp->rcp_reverse_bottleneck_rate  = 0;
+    tp->rcp_rtt                      = 0;  
+    tp->rcp_minRTT                   = ~0;
+    tp->rcp_sRTT                     = 0;
+    
 	tcp_sack_reset(&tp->rx_opt);
 	__sk_dst_reset(sk);
 
Index: linux-2.6.16/net/ipv4/ip_output.c
===================================================================
--- linux-2.6.16/net/ipv4/ip_output.c	(revision 155)
+++ linux-2.6.16/net/ipv4/ip_output.c	(revision 300)
@@ -84,6 +84,8 @@
 #include <linux/netlink.h>
 #include <linux/tcp.h>
 
+#define debug_ip_output 0 
+
 int sysctl_ip_default_ttl = IPDEFTTL;
 
 static int ip_fragment(struct sk_buff *skb, int (*output)(struct sk_buff*));
@@ -91,6 +93,8 @@
 /* Generate a checksum for an outgoing IP datagram. */
 __inline__ void ip_send_check(struct iphdr *iph)
 {
+    if (debug_ip_output)
+        printk("--> ip_send_check [ip_output.c] \n");
 	iph->check = 0;
 	iph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);
 }
@@ -127,6 +131,9 @@
 	struct rtable *rt = (struct rtable *)skb->dst;
 	struct iphdr *iph;
 
+    if (debug_ip_output)
+        printk("--> ip_build_and_send_pkt [ip_output.c] \n");
+
 	/* Build the IP header. */
 	if (opt)
 		iph=(struct iphdr *)skb_push(skb,sizeof(struct iphdr) + opt->optlen);
@@ -144,6 +151,15 @@
 	iph->daddr    = rt->rt_dst;
 	iph->saddr    = rt->rt_src;
 	iph->protocol = sk->sk_protocol;
+
+    /* Nandita: start */
+    if (sk->sk_protocol == IPPROTO_TCP) {
+        iph->protocol = IPPROTO_RCP;
+    } else {
+        iph->protocol = sk->sk_protocol;
+    }
+    /* Nandita: end */
+
 	iph->tot_len  = htons(skb->len);
 	ip_select_ident(iph, &rt->u.dst, sk);
 	skb->nh.iph   = iph;
@@ -170,6 +186,9 @@
 	struct net_device *dev = dst->dev;
 	int hh_len = LL_RESERVED_SPACE(dev);
 
+    if (debug_ip_output)
+        printk("--> ip_finish_output2 [ip_output.c] \n");
+
 	/* Be paranoid, rather than too clever. */
 	if (unlikely(skb_headroom(skb) < hh_len && dev->hard_header)) {
 		struct sk_buff *skb2;
@@ -205,6 +224,10 @@
 
 static inline int ip_finish_output(struct sk_buff *skb)
 {
+
+    if (debug_ip_output)
+        printk("--> ip_finish_output [ip_output.c] \n"); 
+
 #if defined(CONFIG_NETFILTER) && defined(CONFIG_XFRM)
 	/* Policy lookup after SNAT yielded a new policy */
 	if (skb->dst->xfrm != NULL) {
@@ -225,6 +248,9 @@
 	struct rtable *rt = (struct rtable*)skb->dst;
 	struct net_device *dev = rt->u.dst.dev;
 
+    if (debug_ip_output)
+        printk("--> ip_mc_output [ip_output.c] \n");
+
 	/*
 	 *	If the indicated interface is up and running, send the packet.
 	 */
@@ -282,6 +308,9 @@
 {
 	struct net_device *dev = skb->dst->dev;
 
+    if (debug_ip_output)
+        printk("--> ++ ip_output [ip_output.c] \n");
+
 	IP_INC_STATS(IPSTATS_MIB_OUTREQUESTS);
 
 	skb->dev = dev;
@@ -300,6 +329,9 @@
 	struct rtable *rt;
 	struct iphdr *iph;
 
+    if (debug_ip_output)
+        printk("--> ip_queue_xmit [ip_output.c] \n");
+
 	/* Skip all of this if the packet is already routed,
 	 * f.e. by something like SCTP.
 	 */
@@ -353,6 +385,15 @@
 		iph->frag_off = 0;
 	iph->ttl      = ip_select_ttl(inet, &rt->u.dst);
 	iph->protocol = sk->sk_protocol;
+
+    /* Nandita: start */
+    if (sk->sk_protocol == IPPROTO_TCP) {
+        iph->protocol = IPPROTO_RCP;
+    } else {
+        iph->protocol = sk->sk_protocol;
+    }
+    /* Nandita: end */
+    
 	iph->saddr    = rt->rt_src;
 	iph->daddr    = rt->rt_dst;
 	skb->nh.iph   = iph;
@@ -383,6 +424,9 @@
 
 static void ip_copy_metadata(struct sk_buff *to, struct sk_buff *from)
 {
+    if (debug_ip_output)
+        printk("--> ip_copy_metadata [ip_output.c] \n");
+
 	to->pkt_type = from->pkt_type;
 	to->priority = from->priority;
 	to->protocol = from->protocol;
@@ -434,6 +478,9 @@
 	struct rtable *rt = (struct rtable*)skb->dst;
 	int err = 0;
 
+    if (debug_ip_output)
+        printk("--> ip_fragment [ip_output.c] \n");
+
 	dev = rt->u.dst.dev;
 
 	/*
@@ -678,6 +725,9 @@
 {
 	struct iovec *iov = from;
 
+    if (debug_ip_output)
+        printk("--> ip_generic_getfrag [ip_output.c] \n");
+
 	if (skb->ip_summed == CHECKSUM_HW) {
 		if (memcpy_fromiovecend(to, iov, offset, len) < 0)
 			return -EFAULT;
@@ -695,6 +745,10 @@
 {
 	char *kaddr;
 	unsigned int csum;
+
+    if (debug_ip_output)
+        printk("--> csum_page [ip_output.c] \n");
+    
 	kaddr = kmap(page);
 	csum = csum_partial(kaddr + offset, copy, 0);
 	kunmap(page);
@@ -710,6 +764,9 @@
 	struct sk_buff *skb;
 	int err;
 
+    if (debug_ip_output)
+        printk("--> ip_ufo_append_data [ip_output.c] \n");
+
 	/* There is support for UDP fragmentation offload by network
 	 * device, so create one single skb packet containing complete
 	 * udp datagram
@@ -786,6 +843,9 @@
 	unsigned int maxfraglen, fragheaderlen;
 	int csummode = CHECKSUM_NONE;
 
+    if (debug_ip_output)
+        printk("--> ip_append_data [ip_output.c] \n");
+
 	if (flags&MSG_PROBE)
 		return 0;
 
@@ -1054,6 +1114,9 @@
 	int err;
 	unsigned int maxfraglen, fragheaderlen, fraggap;
 
+    if (debug_ip_output)
+        printk("--> ip_append_page [ip_output.c] \n");
+
 	if (inet->hdrincl)
 		return -EPERM;
 
@@ -1197,6 +1260,9 @@
 	__u8 ttl;
 	int err = 0;
 
+    if (debug_ip_output)
+        printk("--> ip_push_pending_frames [ip_output.c] \n");
+
 	if ((skb = __skb_dequeue(&sk->sk_write_queue)) == NULL)
 		goto out;
 	tail_skb = &(skb_shinfo(skb)->frag_list);
@@ -1296,6 +1362,9 @@
 	struct inet_sock *inet = inet_sk(sk);
 	struct sk_buff *skb;
 
+    if (debug_ip_output)
+        printk("--> ip_flush_pending_frames [ip_output.c] \n");
+
 	while ((skb = __skb_dequeue_tail(&sk->sk_write_queue)) != NULL)
 		kfree_skb(skb);
 
@@ -1317,6 +1386,9 @@
 {
 	unsigned int csum;
 
+    if (debug_ip_output)
+        printk("--> ip_reply_glue_bits [ip_output.c] \n");
+
 	csum = csum_partial_copy_nocheck(dptr+offset, to, len, 0);
 	skb->csum = csum_block_add(skb->csum, csum, odd);
 	return 0;  
@@ -1343,6 +1415,9 @@
 	u32 daddr;
 	struct rtable *rt = (struct rtable*)skb->dst;
 
+    if (debug_ip_output)
+        printk("--> ip_send_reply [ip_output.c] \n");
+
 	if (ip_options_echo(&replyopts.opt, skb))
 		return;
 
@@ -1380,6 +1455,14 @@
 	inet->tos = skb->nh.iph->tos;
 	sk->sk_priority = skb->priority;
 	sk->sk_protocol = skb->nh.iph->protocol;
+
+    /* Nandita: start */
+    if (skb->nh.iph->protocol == IPPROTO_RCP)
+        sk->sk_protocol = IPPROTO_TCP;
+    else
+        sk->sk_protocol = skb->nh.iph->protocol;
+    /* Nandita: end */
+    
 	ip_append_data(sk, ip_reply_glue_bits, arg->iov->iov_base, len, 0,
 		       &ipc, rt, MSG_DONTWAIT);
 	if ((skb = skb_peek(&sk->sk_write_queue)) != NULL) {
Index: linux-2.6.16/net/ipv4/tcp_cong.c
===================================================================
--- linux-2.6.16/net/ipv4/tcp_cong.c	(revision 155)
+++ linux-2.6.16/net/ipv4/tcp_cong.c	(revision 300)
@@ -12,15 +12,21 @@
 #include <linux/types.h>
 #include <linux/list.h>
 #include <net/tcp.h>
+#include <linux/rcp.h>
 
 static DEFINE_SPINLOCK(tcp_cong_list_lock);
 static LIST_HEAD(tcp_cong_list);
 
+#define debug_tcp_cong 1
+
 /* Simple linear search, don't expect many entries! */
 static struct tcp_congestion_ops *tcp_ca_find(const char *name)
 {
 	struct tcp_congestion_ops *e;
 
+    if (debug_tcp_cong)
+        printk("tcp_ca_find [tcp_cong.c] \n");
+
 	list_for_each_entry_rcu(e, &tcp_cong_list, list) {
 		if (strcmp(e->name, name) == 0)
 			return e;
@@ -37,6 +43,9 @@
 {
 	int ret = 0;
 
+    if (debug_tcp_cong)
+        printk("tcp_register_congestion_control [tcp_cong.c] \n");
+
 	/* all algorithms must implement ssthresh and cong_avoid ops */
 	if (!ca->ssthresh || !ca->cong_avoid || !ca->min_cwnd) {
 		printk(KERN_ERR "TCP %s does not implement required ops\n",
@@ -66,6 +75,8 @@
  */
 void tcp_unregister_congestion_control(struct tcp_congestion_ops *ca)
 {
+    if (debug_tcp_cong)
+        printk("tcp_unregister_congestion_control [tcp_cong.c] \n");
 	spin_lock(&tcp_cong_list_lock);
 	list_del_rcu(&ca->list);
 	spin_unlock(&tcp_cong_list_lock);
@@ -78,6 +89,9 @@
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	struct tcp_congestion_ops *ca;
 
+    if (debug_tcp_cong)
+        printk("tcp_init_congestion_control [tcp_cong.c] \n");
+
 	if (icsk->icsk_ca_ops != &tcp_init_congestion_ops)
 		return;
 
@@ -100,6 +114,9 @@
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
 
+    if (debug_tcp_cong)
+        printk("tcp_cleanup_congestion_control [tcp_cong.c] \n");
+
 	if (icsk->icsk_ca_ops->release)
 		icsk->icsk_ca_ops->release(sk);
 	module_put(icsk->icsk_ca_ops->owner);
@@ -111,6 +128,9 @@
 	struct tcp_congestion_ops *ca;
 	int ret = -ENOENT;
 
+    if (debug_tcp_cong)
+        printk("tcp_set_default_congestion_control [tcp_cong.c] \n");
+
 	spin_lock(&tcp_cong_list_lock);
 	ca = tcp_ca_find(name);
 #ifdef CONFIG_KMOD
@@ -136,6 +156,10 @@
 void tcp_get_default_congestion_control(char *name)
 {
 	struct tcp_congestion_ops *ca;
+
+    if (debug_tcp_cong)
+        printk("tcp_get_default_congestion_control [tcp_cong.c] \n");
+    
 	/* We will always have reno... */
 	BUG_ON(list_empty(&tcp_cong_list));
 
@@ -152,6 +176,9 @@
 	struct tcp_congestion_ops *ca;
 	int err = 0;
 
+    if (debug_tcp_cong)
+        printk("tcp_set_congestion_control [tcp_cong.c] \n");
+
 	rcu_read_lock();
 	ca = tcp_ca_find(name);
 	if (ca == icsk->icsk_ca_ops)
@@ -180,6 +207,14 @@
  */
 void tcp_slow_start(struct tcp_sock *tp)
 {
+
+    /* Nandita */
+    char tcp_cong_name[TCP_CA_NAME_MAX];
+    tcp_get_default_congestion_control(tcp_cong_name);
+    
+    if (debug_tcp_cong)
+        printk("tcp_slow_start [tcp_cong.c] \n");
+
 	if (sysctl_tcp_abc) {
 		/* RFC3465: Slow Start
 		 * TCP sender SHOULD increase cwnd by the number of
@@ -189,16 +224,25 @@
 		if (tp->bytes_acked < tp->mss_cache)
 			return;
 
-		/* We MAY increase by 2 if discovered delayed ack */
-		if (sysctl_tcp_abc > 1 && tp->bytes_acked > 2*tp->mss_cache) {
-			if (tp->snd_cwnd < tp->snd_cwnd_clamp)
-				tp->snd_cwnd++;
-		}
+        if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) {
+            tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+            
+        } else {
+		    /* We MAY increase by 2 if discovered delayed ack */
+		    if (sysctl_tcp_abc > 1 && tp->bytes_acked > 2*tp->mss_cache) {
+			    if (tp->snd_cwnd < tp->snd_cwnd_clamp)
+				    tp->snd_cwnd++;
+		    }
+        }
 	}
 	tp->bytes_acked = 0;
 
-	if (tp->snd_cwnd < tp->snd_cwnd_clamp)
-		tp->snd_cwnd++;
+    if ((strcmp(tcp_cong_name, "rcp") == 0) && (tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) 
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+    else {
+	    if (tp->snd_cwnd < tp->snd_cwnd_clamp)
+		    tp->snd_cwnd++;
+    }
 }
 EXPORT_SYMBOL_GPL(tcp_slow_start);
 
@@ -214,6 +258,9 @@
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+    if (debug_tcp_cong)
+        printk("tcp_reno_cong_avoid [tcp_cong.c] \n");
+
 	if (!tcp_is_cwnd_limited(sk, in_flight))
 		return;
 
@@ -247,6 +294,9 @@
 u32 tcp_reno_ssthresh(struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_cong)
+        printk("tcp_reno_ssthresh [tcp_cong.c] \n");
 	return max(tp->snd_cwnd >> 1U, 2U);
 }
 EXPORT_SYMBOL_GPL(tcp_reno_ssthresh);
@@ -255,6 +305,9 @@
 u32 tcp_reno_min_cwnd(struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_cong)
+        printk("tcp_reno_min_cwnd [tcp_cong.c] \n");
 	return tp->snd_ssthresh/2;
 }
 EXPORT_SYMBOL_GPL(tcp_reno_min_cwnd);
Index: linux-2.6.16/net/ipv4/tcp_bic.c
===================================================================
--- linux-2.6.16/net/ipv4/tcp_bic.c	(revision 155)
+++ linux-2.6.16/net/ipv4/tcp_bic.c	(revision 300)
@@ -50,7 +50,7 @@
 /* BIC TCP Parameters */
 struct bictcp {
 	u32	cnt;		/* increase cwnd by 1 after ACKs */
-	u32 	last_max_cwnd;	/* last maximum snd_cwnd */
+	u32 last_max_cwnd;	/* last maximum snd_cwnd */
 	u32	loss_cwnd;	/* congestion window at last loss */
 	u32	last_cwnd;	/* the last snd_cwnd */
 	u32	last_time;	/* time when updated last_cwnd */
@@ -61,6 +61,7 @@
 
 static inline void bictcp_reset(struct bictcp *ca)
 {
+    printk("bictcp_reset() [tcp_bic.c] \n");
 	ca->cnt = 0;
 	ca->last_max_cwnd = 0;
 	ca->loss_cwnd = 0;
@@ -72,6 +73,7 @@
 
 static void bictcp_init(struct sock *sk)
 {
+    printk("bictcp_init() [tcp_bic.c] \n");
 	bictcp_reset(inet_csk_ca(sk));
 	if (initial_ssthresh)
 		tcp_sk(sk)->snd_ssthresh = initial_ssthresh;
@@ -82,6 +84,8 @@
  */
 static inline void bictcp_update(struct bictcp *ca, u32 cwnd)
 {
+    printk("bictcp_update() [tcp_bic.c] \n");
+
 	if (ca->last_cwnd == cwnd &&
 	    (s32)(tcp_time_stamp - ca->last_time) <= HZ / 32)
 		return;
@@ -143,6 +147,8 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bictcp *ca = inet_csk_ca(sk);
 
+    printk("bictcp_cong_avoid() [tcp_bic.c] \n");
+
 	if (!tcp_is_cwnd_limited(sk, in_flight))
 		return;
 
@@ -173,6 +179,8 @@
 	const struct tcp_sock *tp = tcp_sk(sk);
 	struct bictcp *ca = inet_csk_ca(sk);
 
+    printk("bictcp_recalc_ssthresh() [tcp_bic.c] \n");
+
 	ca->epoch_start = 0;	/* end of epoch */
 
 	/* Wmax and fast convergence */
@@ -195,17 +203,22 @@
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
 	const struct bictcp *ca = inet_csk_ca(sk);
+
+    printk("bictcp_undo_cwnd() [tcp_bic.c] \n");
 	return max(tp->snd_cwnd, ca->last_max_cwnd);
 }
 
 static u32 bictcp_min_cwnd(struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
+    printk("bictcp_min_cwnd() [tcp_bic.c] \n");
 	return tp->snd_ssthresh;
 }
 
 static void bictcp_state(struct sock *sk, u8 new_state)
 {
+    printk("bictcp_state() [tcp_bic.c] \n");
+
 	if (new_state == TCP_CA_Loss)
 		bictcp_reset(inet_csk_ca(sk));
 }
@@ -216,6 +229,7 @@
 static void bictcp_acked(struct sock *sk, u32 cnt)
 {
 	const struct inet_connection_sock *icsk = inet_csk(sk);
+    printk("bictcp_acked() [tcp_bic.c] \n");
 
 	if (cnt > 0 && icsk->icsk_ca_state == TCP_CA_Open) {
 		struct bictcp *ca = inet_csk_ca(sk);
@@ -239,12 +253,14 @@
 
 static int __init bictcp_register(void)
 {
+    printk("__init bictcp_register() [tcp_bic.c] \n");
 	BUG_ON(sizeof(struct bictcp) > ICSK_CA_PRIV_SIZE);
 	return tcp_register_congestion_control(&bictcp);
 }
 
 static void __exit bictcp_unregister(void)
 {
+    printk("__exit bictcp_reset() [tcp_bic.c] \n");
 	tcp_unregister_congestion_control(&bictcp);
 }
 
Index: linux-2.6.16/net/ipv4/ip_input.c
===================================================================
--- linux-2.6.16/net/ipv4/ip_input.c	(revision 155)
+++ linux-2.6.16/net/ipv4/ip_input.c	(revision 300)
@@ -147,6 +147,9 @@
 #include <linux/mroute.h>
 #include <linux/netlink.h>
 
+/* Nandita */
+#define debug_ip_input 0 
+
 /*
  *	SNMP management statistics
  */
@@ -162,6 +165,9 @@
 	u8 protocol = skb->nh.iph->protocol;
 	struct sock *last = NULL;
 
+    if (debug_ip_input)
+        printk("<-- ip_call_ra_chain [ip_input.c] \n");
+
 	read_lock(&ip_ra_lock);
 	for (ra = ip_ra_chain; ra; ra = ra->next) {
 		struct sock *sk = ra->sk;
@@ -201,6 +207,9 @@
 {
 	int ihl = skb->nh.iph->ihl*4;
 
+    if (debug_ip_input)
+        printk("<-- ip_local_deliver_finish [ip_input.c] \n");
+
 	__skb_pull(skb, ihl);
 
         /* Point into the IP datagram, just past the header. */
@@ -266,6 +275,8 @@
 	/*
 	 *	Reassemble IP fragments.
 	 */
+    if (debug_ip_input) 
+        printk("<-- ip_local_deliver [ip_input.c] \n");
 
 	if (skb->nh.iph->frag_off & htons(IP_MF|IP_OFFSET)) {
 		skb = ip_defrag(skb, IP_DEFRAG_LOCAL_DELIVER);
@@ -283,6 +294,9 @@
 	struct iphdr *iph;
 	struct net_device *dev = skb->dev;
 
+    if (debug_ip_input)
+        printk("<-- ip_rcv_options [ip_input.c] \n");
+
 	/* It looks as overkill, because not all
 	   IP options require packet mangling.
 	   But it is the easiest for now, especially taking
@@ -333,6 +347,8 @@
 {
 	struct iphdr *iph = skb->nh.iph;
 
+    if (debug_ip_input)
+        printk("<-- ip_rcv_finish [ip_input.c] \n");
 	/*
 	 *	Initialise the virtual path cache for the packet. It describes
 	 *	how the packet travels inside Linux networking.
@@ -376,6 +392,9 @@
 	struct iphdr *iph;
 	u32 len;
 
+    if (debug_ip_input)
+        printk("<-- ip_rcv [ip_input.c] \n");
+
 	/* When the interface is in promisc. mode, drop all the crap
 	 * that it receives, do not try to analyse it.
 	 */
Index: linux-2.6.16/net/ipv4/tcp_rcp.c
===================================================================
--- linux-2.6.16/net/ipv4/tcp_rcp.c	(revision 0)
+++ linux-2.6.16/net/ipv4/tcp_rcp.c	(revision 300)
@@ -0,0 +1,352 @@
+/*
+ * TCP using congestion information of RCP
+ */
+
+#include <linux/config.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <net/tcp.h>
+
+#include <linux/rcp.h>
+
+
+#define RCPTCP_BETA_SCALE    1024	/* Scale factor beta calculation
+					 * max_cwnd = snd_cwnd * beta
+					 */
+#define RCPTCP_B		4	 /*
+					  * In binary search,
+					  * go to point (max+min)/N
+					  */
+#define debug_tcp_rcp 1
+#define RCP_HEADER_SIZE 12
+#define IP_HEADER_SIZE 20
+
+static int fast_convergence = 1;
+static int max_increment = 16;
+static int low_window = 14;
+static int beta = 819;		/* = 819/1024 (RCPTCP_BETA_SCALE) */
+static int initial_ssthresh = 100;
+static int smooth_part = 20;
+
+module_param(fast_convergence, int, 0644);
+MODULE_PARM_DESC(fast_convergence, "turn on/off fast convergence");
+module_param(max_increment, int, 0644);
+MODULE_PARM_DESC(max_increment, "Limit on increment allowed during binary search");
+module_param(low_window, int, 0644);
+MODULE_PARM_DESC(low_window, "lower bound on congestion window (for TCP friendliness)");
+module_param(beta, int, 0644);
+MODULE_PARM_DESC(beta, "beta for multiplicative increase");
+module_param(initial_ssthresh, int, 0644);
+MODULE_PARM_DESC(initial_ssthresh, "initial value of slow start threshold");
+module_param(smooth_part, int, 0644);
+MODULE_PARM_DESC(smooth_part, "log(B/(B*Smin))/log(B/(B-1))+B, # of RTT from Wmax-B to Wmax");
+
+
+/* RCP TCP Parameters */
+struct rcptcp {
+	u32	cnt;		/* increase cwnd by 1 after ACKs */
+	u32	last_max_cwnd;	/* last maximum snd_cwnd */
+	u32	loss_cwnd;	/* congestion window at last loss */
+	u32	last_cwnd;	/* the last snd_cwnd */
+	u32	last_time;	/* time when updated last_cwnd */
+	u32	epoch_start;	/* beginning of an epoch */
+#define ACK_RATIO_SHIFT	4
+	u32	delayed_ack;	/* estimate the ratio of Packets/ACKs << 4 */
+};
+
+static inline void rcptcp_reset(struct rcptcp *ca)
+{
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_reset [tcp_rcp.c]\n");
+
+	ca->cnt = 0;
+	ca->last_max_cwnd = 0;
+	ca->loss_cwnd = 0;
+	ca->last_cwnd = 0;
+	ca->last_time = 0;
+	ca->epoch_start = 0;
+	ca->delayed_ack = 2 << ACK_RATIO_SHIFT;
+}
+
+static void rcptcp_init(struct sock *sk)
+{
+
+    struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_init [tcp_rcp.c]\n");
+
+	rcptcp_reset(inet_csk_ca(sk));
+	if (initial_ssthresh)
+		tcp_sk(sk)->snd_ssthresh = initial_ssthresh;
+
+    if ((tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+
+    printk(" +++ rcptcp_init: snd_cwnd %u \n", tcp_sk(sk)->snd_cwnd);
+}
+
+/*
+ * Compute congestion window to use.
+ */
+static inline void rcptcp_update(struct rcptcp *ca, u32 cwnd)
+{
+
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_update [tcp_rcp.c]\n");
+    
+	if (ca->last_cwnd == cwnd &&
+	    (s32)(tcp_time_stamp - ca->last_time) <= HZ / 32)
+		return;
+
+	ca->last_cwnd = cwnd;
+	ca->last_time = tcp_time_stamp;
+
+	if (ca->epoch_start == 0) /* record the beginning of an epoch */
+		ca->epoch_start = tcp_time_stamp;
+
+	/* start off normal */
+	if (cwnd <= low_window) {
+		ca->cnt = cwnd;
+		return;
+	}
+
+	/* binary increase */
+	if (cwnd < ca->last_max_cwnd) {
+		__u32 	dist = (ca->last_max_cwnd - cwnd)
+			/ RCPTCP_B;
+
+		if (dist > max_increment)
+			/* linear increase */
+			ca->cnt = cwnd / max_increment;
+		else if (dist <= 1U)
+			/* binary search increase */
+			ca->cnt = (cwnd * smooth_part) / RCPTCP_B;
+		else
+			/* binary search increase */
+			ca->cnt = cwnd / dist;
+	} else {
+		/* slow start AMD linear increase */
+		if (cwnd < ca->last_max_cwnd + RCPTCP_B)
+			/* slow start */
+			ca->cnt = (cwnd * smooth_part) / RCPTCP_B;
+		else if (cwnd < ca->last_max_cwnd + max_increment*(RCPTCP_B-1))
+			/* slow start */
+			ca->cnt = (cwnd * (RCPTCP_B-1))
+				/ (cwnd - ca->last_max_cwnd);
+		else
+			/* linear increase */
+			ca->cnt = cwnd / max_increment;
+	}
+
+	/* if in slow start or link utilization is very low */
+	if (ca->loss_cwnd == 0) {
+		if (ca->cnt > 20) /* increase cwnd 5% per RTT */
+			ca->cnt = 20;
+	}
+
+	ca->cnt = (ca->cnt << ACK_RATIO_SHIFT) / ca->delayed_ack;
+	if (ca->cnt == 0)			/* cannot be zero */
+		ca->cnt = 1;
+}
+
+static void rcptcp_cong_avoid(struct sock *sk, u32 ack,
+			      u32 seq_rtt, u32 in_flight, int data_acked)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct rcptcp *ca = inet_csk_ca(sk);
+
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_cong_avoid [tcp_rcp.c]\n");
+
+    if ((tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) {
+
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+
+    } else {
+
+	    if (!tcp_is_cwnd_limited(sk, in_flight))
+		    return;
+
+	    if (tp->snd_cwnd <= tp->snd_ssthresh)
+		    tcp_slow_start(tp);
+	    else {
+		    rcptcp_update(ca, tp->snd_cwnd);
+
+		    /* In dangerous area, increase slowly.
+		     * In theory this is tp->snd_cwnd += 1 / tp->snd_cwnd
+		     */
+		    if (tp->snd_cwnd_cnt >= ca->cnt) {
+			    if (tp->snd_cwnd < tp->snd_cwnd_clamp)
+				    tp->snd_cwnd++;
+			    tp->snd_cwnd_cnt = 0;
+		    } else
+			    tp->snd_cwnd_cnt++;
+	    }
+    }
+
+    printk(" +++ rcptcp_cong_avoid(): snd_cwnd %u \n", tcp_sk(sk)->snd_cwnd);
+
+}
+
+static void rcptcp_rtt_calc(struct sock *sk, u32 usrtt)
+{
+
+    /* Updates RTT and recomputes snd_cwnd */ 
+
+    struct tcp_sock *tp = tcp_sk(sk);
+	u16 msrtt = max(usrtt/1000, 1) ; /* RTT in msecs. Don't allow zero rtt */
+
+	/* Filter to find propagation delay: */
+	if (msrtt < tp->rcp_minRTT)
+		tp->rcp_minRTT = msrtt;
+
+    tp->rcp_rtt = tp->rcp_minRTT;
+
+    /* Smoothed RTT */
+    tp->rcp_sRTT = max(tp->rcp_minRTT, (7*tp->rcp_sRTT >> 3) + (msrtt >> 3));  
+
+    if ((tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0))
+        tp->snd_cwnd = max(1, (tp->rcp_bottleneck_rate * tp->rcp_rtt)/max(tp->mss_cache + tp->tcp_header_len + sizeof(struct rcphdr) + IP_HEADER_SIZE, 1));
+
+    printk(" +++ rcptcp_rtt_calc(): msrtt %u, tp->rcp_minRTT %u, tp->rcp_sRTT %u tp->srtt %u (msecs) \n", msrtt, tp->rcp_minRTT, tp->rcp_sRTT, jiffies_to_msecs(tp->srtt));
+}
+
+
+
+/*
+ *	behave like Reno until low_window is reached,
+ *	then increase congestion window slowly
+ */
+static u32 rcptcp_recalc_ssthresh(struct sock *sk)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	struct rcptcp *ca = inet_csk_ca(sk);
+
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_recalc_ssthresh [tcp_rcp.c]\n");
+
+    if ((tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) {
+
+        printk(" +++ rcptcp_recalc_ssthresh(): snd_cwnd %u \n", tcp_sk(sk)->snd_cwnd);
+        return (tp->snd_cwnd); 
+
+    } else {
+    
+	    ca->epoch_start = 0;	/* end of epoch */
+
+	    /* Wmax and fast convergence */
+	    if (tp->snd_cwnd < ca->last_max_cwnd && fast_convergence)
+		    ca->last_max_cwnd = (tp->snd_cwnd * (RCPTCP_BETA_SCALE + beta)) / (2 * RCPTCP_BETA_SCALE);
+	    else
+		    ca->last_max_cwnd = tp->snd_cwnd;
+
+	    ca->loss_cwnd = tp->snd_cwnd;
+
+        printk(" +++ rcptcp_recalc_ssthresh(): snd_cwnd %u \n", tcp_sk(sk)->snd_cwnd);
+
+	    if (tp->snd_cwnd <= low_window)
+		    return max(tp->snd_cwnd >> 1U, 2U);
+	    else
+		    return max((tp->snd_cwnd * beta) / RCPTCP_BETA_SCALE, 2U);
+   }
+}
+
+static u32 rcptcp_undo_cwnd(struct sock *sk)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	const struct rcptcp *ca = inet_csk_ca(sk);
+
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_undo_cwnd [tcp_rcp.c]\n");
+
+    printk(" +++ rcptcp_undo_cwnd(): snd_cwnd %u \n", tcp_sk(sk)->snd_cwnd);
+
+    if ((tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) {
+        return (tp->snd_cwnd); 
+    } else {
+	    return max(tp->snd_cwnd, ca->last_max_cwnd);
+    }
+}
+
+static u32 rcptcp_min_cwnd(struct sock *sk)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_min_cwnd [tcp_rcp.c]\n");
+
+    printk(" +++ rcptcp_min_cwnd(): snd_cwnd %u \n", tcp_sk(sk)->snd_cwnd);
+
+    if ((tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) {
+        return (tp->snd_cwnd);
+    } else {
+	    return tp->snd_ssthresh;
+    }
+}
+
+static void rcptcp_state(struct sock *sk, u8 new_state)
+{
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_state [tcp_rcp.c]\n");
+
+	if (new_state == TCP_CA_Loss)
+		rcptcp_reset(inet_csk_ca(sk));
+}
+
+/* Track delayed acknowledgment ratio using sliding window
+ * ratio = (15*ratio + sample) / 16
+ */
+static void rcptcp_acked(struct sock *sk, u32 cnt)
+{
+	const struct inet_connection_sock *icsk = inet_csk(sk);
+    const struct tcp_sock *tp = tcp_sk(sk);
+
+    if (debug_tcp_rcp)
+        printk(" *** rcptcp_acked [tcp_rcp.c]\n");
+
+    printk(" +++ rcptcp_acked(): snd_cwnd %u \n", tcp_sk(sk)->snd_cwnd);
+    
+    if ((tp->rcp_bottleneck_rate > 0) && (tp->rcp_rtt > 0)) {
+
+    } else {
+    
+	    if (cnt > 0 && icsk->icsk_ca_state == TCP_CA_Open) {
+		    struct rcptcp *ca = inet_csk_ca(sk);
+		    cnt -= ca->delayed_ack >> ACK_RATIO_SHIFT;
+		    ca->delayed_ack += cnt;
+	    }
+    }
+}
+
+
+static struct tcp_congestion_ops rcptcp = {
+	.init		= rcptcp_init,
+	.ssthresh	= rcptcp_recalc_ssthresh,
+	.cong_avoid	= rcptcp_cong_avoid,
+    .rtt_sample = rcptcp_rtt_calc,
+	.set_state	= rcptcp_state,
+	.undo_cwnd	= rcptcp_undo_cwnd,
+	.min_cwnd	= rcptcp_min_cwnd,
+	.pkts_acked = rcptcp_acked,
+	.owner		= THIS_MODULE,
+	.name		= "rcp",
+};
+
+static int __init rcptcp_register(void)
+{
+	BUG_ON(sizeof(struct rcptcp) > ICSK_CA_PRIV_SIZE);
+	return tcp_register_congestion_control(&rcptcp);
+}
+
+static void __exit rcptcp_unregister(void)
+{
+	tcp_unregister_congestion_control(&rcptcp);
+}
+
+module_init(rcptcp_register);
+module_exit(rcptcp_unregister);
+
+MODULE_AUTHOR("Stephen Hemminger");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("RCP TCP");
